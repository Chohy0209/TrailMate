import os
import time
import asyncio
import random
import operator
from typing import TypedDict, Annotated, List, Any, Dict

#naver api
from naver_api import build_snippet_per_doc, format_snippets_as_text

# OpenAI SDK
from openai import OpenAI

# LangChain / Vector store
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, END
from langchain_core.documents import Document
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
import numpy as np
from langchain.schema import Document
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.embeddings.base import Embeddings

# BGE-M3 í†µí•© ì‚¬ìš©
from FlagEmbedding import BGEM3FlagModel

load_dotenv()

# ===== í™˜ê²½ ì„¤ì • =====
# âš ï¸ í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì½ìŠµë‹ˆë‹¤: export OPENAI_API_KEY="sk-..."
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=30.0,
)

EMBEDDING_MODEL_NAME = "dragonkue/BGE-m3-ko"
CHROMA_DB_PATH = "./data/camp_vectorDB_BGE_sentence_per_doc"
MAX_LOOPS = 2

GPT_MODEL = "ft:gpt-4.1-mini-2025-04-14:ailab:camping-rag-qa:C2bHhwJM:ckpt-step-714"

# âœ… ìƒˆë¡œìš´ í†µí•© BGE-M3 í´ë˜ìŠ¤
class UnifiedBGEM3Embedder:
    def __init__(self, model_name="dragonkue/BGE-m3-ko"):
        self.model = None
        self.model_name = model_name
        self._load_model()
    
    def _load_model(self):
        """BGE-M3 ëª¨ë¸ ë¡œë“œ"""
        try:
            self.model = BGEM3FlagModel(self.model_name, use_fp16=True)
            print(f"âœ… BGE-M3 í†µí•© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")
        except Exception as e:
            print(f"âŒ BGE-M3 í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            try:
                # Fallback: ì›ë³¸ BGE-M3 ëª¨ë¸
                self.model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
                print("âœ… BGE-M3 ì›ë³¸ ëª¨ë¸ë¡œ Fallback ë¡œë“œ ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ BGE-M3 Fallbackë„ ì‹¤íŒ¨: {e2}")
                self.model = None
    
    def encode_for_vector_db(self, texts, batch_size=12):
        """ChromaDBìš© dense embedding ìƒì„±"""
        if self.model is None:
            raise Exception("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False
        )
        return embeddings['dense_vecs']
    
    def encode_hybrid(self, query_text, candidate_texts, batch_size=12):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© dense + sparse ë™ì‹œ ê³„ì‚°"""
        if self.model is None:
            raise Exception("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        
        all_texts = [query_text] + candidate_texts
        embeddings = self.model.encode(
            all_texts,
            batch_size=batch_size,
            return_dense=True,
            return_sparse=True,
            return_colbert_vecs=False
        )
        
        return {
            'query_dense': embeddings['dense_vecs'][0],
            'doc_dense': embeddings['dense_vecs'][1:],
            'query_sparse': embeddings['lexical_weights'][0],
            'doc_sparse': embeddings['lexical_weights'][1:]
        }

# âœ… ChromaDBìš© ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜
class BGEM3LangChainEmbeddings(Embeddings):
    def __init__(self, unified_embedder):
        self.unified_embedder = unified_embedder
    
    def embed_documents(self, texts):
        """ë¬¸ì„œë“¤ ì„ë² ë”©"""
        try:
            embeddings = self.unified_embedder.encode_for_vector_db(texts)
            return embeddings.tolist()
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            # Fallback: ë¹ˆ ë²¡í„° ë°˜í™˜
            return [[0.0] * 1024 for _ in texts]
    
    def embed_query(self, text):
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        try:
            embeddings = self.unified_embedder.encode_for_vector_db([text])
            return embeddings[0].tolist()
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            # Fallback: ë¹ˆ ë²¡í„° ë°˜í™˜
            return [0.0] * 1024

# âœ… í†µí•© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
unified_embedder = UnifiedBGEM3Embedder(EMBEDDING_MODEL_NAME)

# ===== ìƒíƒœ ì •ì˜ =====
class GraphState(TypedDict):
    question: str
    original_question: str
    classification: str
    camping_type_preference: str
    context: Annotated[List[Any], operator.add]
    locations: Annotated[List[dict], operator.add]
    final_answer: str
    loop_count: int
    search_attempted: bool
    error_message: str

# ===== ê³µìš© LLM í˜¸ì¶œ ìœ í‹¸ =====

def oai_text(prompt: str, model: str = GPT_MODEL) -> Dict[str, Any]:
    """Responses APIë¡œ í…ìŠ¤íŠ¸ë§Œ ë°›ì•„ì˜¤ëŠ” í—¬í¼.
    ë°˜í™˜: {"text": str, "request_id": str}
    """
    resp = client.responses.create(
        model=model,
        input=prompt,
    )
    # ê³µì‹ SDKëŠ” output_text ì œê³µ â†’ íŒŒì‹± ì¸ë±ìŠ¤ ì‹¤ìˆ˜ ë°©ì§€
    return {"text": (resp.output_text or "").strip(), "request_id": getattr(resp, "_request_id", None)}

# ===== ì„ë² ë”© ë° DB ë¡œë”© =====
print("--- ChromaDB ë¡œë”© ì¤‘ ---")
start_time = time.perf_counter()

# âœ… ë³€ê²½: í†µí•© ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©
embedding_function = BGEM3LangChainEmbeddings(unified_embedder)
vectordb = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embedding_function)

end_time = time.perf_counter()
print(f"--- ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")

# --- 1ì°¨ ë¶„ë¥˜ ë…¸ë“œ ---

def classify_question_type(state: GraphState) -> dict:
    t0 = time.perf_counter()
    question = state["question"]
    prompt = (
        "ë‹¹ì‹ ì€ ìº í•‘ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë†€ëŸ¬ê°ˆ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë‹¬ë¼ê³  í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  ì¼ë°˜ ìº í•‘ì— ê´€í•œ ì§ˆë¬¸ì´ë©´ 'ì¼ë°˜ ìº í•‘' ìœ¼ë¡œ ë‹µí•˜ê³ , ë†€ëŸ¬ê°€ëŠ” ì¥ì†Œì— ê´€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ë§¥ìƒ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì€ 'ì¥ì†Œ ì¶”ì²œ'ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n" 
        "'ì¼ë°˜ ìº í•‘', 'ì¥ì†Œ ì¶”ì²œ' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n"
        "ë¶„ë¥˜:"
    )
    try:
        out = oai_text(prompt)
        text = out["text"]

        categories = ["ì¼ë°˜ ìº í•‘", "ì¥ì†Œ ì¶”ì²œ"]

        # ë¬¸ì¥ ì „ì²´ì—ì„œ ì¹´í…Œê³ ë¦¬ ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
        classification = next((cat for cat in categories if cat in text), "ì¼ë°˜ ìº í•‘")

        print(f"ë¶„ë¥˜: {classification} ({time.perf_counter() - t0:.2f}s)")
        return {"classification": classification, "original_question": question}

    except Exception as e:
        print(f"ë¶„ë¥˜ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"classification": "ì¼ë°˜ ìº í•‘", "original_question": question, "error_message": str(e)}
        
# --- ì¼ë°˜ ì§ˆë¬¸ ë‹µë³€ ë…¸ë“œ ---

def generate_general_answer(state: GraphState) -> dict:
    t0 = time.perf_counter()
    question = state["question"]
    prompt = (
        "ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìº í•‘ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    )
    try:
        out = oai_text(prompt)
        answer = out["text"]
        print(f"ì¼ë°˜ì§ˆë¬¸ ì‘ë‹µ ({time.perf_counter() - t0:.2f}s | req={out['request_id']})")
        return {"final_answer": answer}
    except Exception as e:
        print(f"ì¼ë°˜ì§ˆë¬¸ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e)}

# --- ë¼ìš°íŒ… í•¨ìˆ˜ ---

def route_by_classification(state):
    return "general" if state.get("classification") == "ì¼ë°˜ ìº í•‘" else "ask_preference"

def route_by_camping_type(state):
    mapping = {
        "ìœ ë£Œìº í•‘ì¥": "paid",
        "ê¸€ë¨í•‘/ì¹´ë¼ë°˜": "glamping",
        "ì˜¤ì§€/ë…¸ì§€ìº í•‘": "ojee",
    }
    return mapping.get(state.get("camping_type_preference", "ìœ ë£Œìº í•‘ì¥"), "paid")

# --- ìº í•‘ ìœ í˜• ì„ íƒ ìš”ì²­ ë…¸ë“œ ---

def ask_camping_preference(state: GraphState) -> dict:
    message = (
        "ğŸ•ï¸ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”! ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?\n\n"
        "1ï¸âƒ£ ìœ ë£Œìº í•‘ì¥ (ì˜¤í† ìº í•‘ì¥, í¸ì˜ì‹œì„¤ ì™„ë¹„)  \n"
        "2ï¸âƒ£ ê¸€ë¨í•‘/ì¹´ë¼ë°˜ (ëŸ­ì…”ë¦¬, í¸ì•ˆí•œ ìº í•‘)  \n"
        "3ï¸âƒ£ ì˜¤ì§€/ë…¸ì§€ìº í•‘ (ìì—° ì† ë¬´ë£Œ ìº í•‘)  \n\n"
        "ì¸ì›ì´ë‚˜ ìŠ¤íƒ€ì¼ ë“± ëª¨ë‘ ì•Œë ¤ì£¼ì„¸ìš”!"
    )
    return {"final_answer": message}

# --- ìº í•‘ ìœ í˜• ë¶„ë¥˜ ë…¸ë“œ ---
def classify_camping_type(state: GraphState) -> dict:
    t0 = time.perf_counter()
    user_input = state["question"]  # ì‚¬ìš©ìì˜ ë‹µë³€
    original_question = state.get("original_question", "")
    
    prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ì„ í˜¸ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìì˜ ì‘ë‹µê³¼ ì›ë˜ ì§ˆë¬¸ì„ ì½ê³  'ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘/ì¹´ë¼ë°˜', 'ì˜¤ì§€/ë…¸ì§€ìº í•‘' ì¤‘ í•˜ë‚˜ë¡œ ì •í™•í•˜ê²Œ ì¹´í…Œê³ ë¦¬ë§Œì„ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì›ë˜ ì§ˆë¬¸: {original_question}\n"
        f"ì‚¬ìš©ì ì‘ë‹µ: {user_input}\n"
        "ë¶„ë¥˜:"
    )
    
    try:
        out = oai_text(prompt)
        text = out["text"]

        categories = ["ìœ ë£Œìº í•‘ì¥", "ê¸€ë¨í•‘/ì¹´ë¼ë°˜", "ì˜¤ì§€/ë…¸ì§€ìº í•‘"]
        camping_type = next((cat for cat in categories if cat in text), "ìœ ë£Œìº í•‘ì¥")

        print(f"ìº í•‘ìœ í˜•: {camping_type} ({time.perf_counter() - t0:.2f}s)")
        return {"camping_type_preference": camping_type}

    except Exception as e:
        print(f"ìº í•‘ìœ í˜• ë¶„ë¥˜ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"camping_type_preference": "ìœ ë£Œìº í•‘ì¥", "error_message": str(e)}

# --- âœ… í†µí•©ëœ RAG + ì›¹ê²€ìƒ‰ ë…¸ë“œ ---
async def search_camping(state: dict, camping_type: str, vectordb) -> dict:
    """ê°œì„ ëœ BGE-M3 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - ì‹¤ì œ ì ìˆ˜ ì‚¬ìš©"""

    search_query = f"{state.get('original_question', '')} {state.get('question', '')}".strip()
    print(f"\n--- ğŸ” {camping_type} ìœ í˜•ìœ¼ë¡œ í†µí•© BGE-M3 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘ ---")
    print(f"ğŸ”¹ [DEBUG] ê²€ìƒ‰ ì¿¼ë¦¬: {search_query}")

    try:
        # 1ë‹¨ê³„: Dense Searchë¡œ ìƒìœ„ 10ê°œ í›„ë³´ ì¶”ì¶œ
        dense_candidates = await asyncio.to_thread(
            vectordb.similarity_search_with_score,  # âœ… ì ìˆ˜ì™€ í•¨ê»˜ ë°˜í™˜
            query=search_query,
            k=10,
            filter={"ìº í•‘ìœ í˜•": camping_type},
        )
        
        print(f"ğŸ”¹ [DEBUG] Dense í›„ë³´ {len(dense_candidates)}ê°œ ì¶”ì¶œ")
        if not dense_candidates:
            return {"locations": []}

        # Dense ê²°ê³¼ ë¶„ë¦¬ (document, score)
        dense_docs = [item[0] for item in dense_candidates]  # Document ê°ì²´
        dense_raw_scores = [item[1] for item in dense_candidates]  # ì‹¤ì œ ìœ ì‚¬ë„ ì ìˆ˜

        # 2ë‹¨ê³„: BGE-M3 Sparse ì ìˆ˜ ê³„ì‚°
        try:
            candidate_texts = [doc.page_content for doc in dense_docs]
            
            print("ğŸ”¹ [DEBUG] BGE-M3 Sparse ì„ë² ë”© ê³„ì‚° ì¤‘...")
            embeddings = unified_embedder.encode_hybrid(search_query, candidate_texts)
            
            query_sparse = embeddings['query_sparse']
            doc_sparse_list = embeddings['doc_sparse']
            
            # âœ… ì‹¤ì œ Sparse ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
            sparse_raw_scores = []
            for doc_sparse in doc_sparse_list:
                score = 0.0
                # ê³µí†µ í† í°ë“¤ì˜ ê°€ì¤‘ì¹˜ ê³±ì˜ í•©
                for token_id, query_weight in query_sparse.items():
                    if token_id in doc_sparse:
                        score += query_weight * doc_sparse[token_id]
                sparse_raw_scores.append(score)
            
            print(f"ğŸ”¹ [DEBUG] Sparse ì›ì‹œ ì ìˆ˜ ë²”ìœ„: {min(sparse_raw_scores):.4f} ~ {max(sparse_raw_scores):.4f}")
            
        except Exception as e:
            print(f"âŒ [ERROR] BGE-M3 Sparse ê³„ì‚° ì‹¤íŒ¨: {e}")
            sparse_raw_scores = [0.0] * len(dense_docs)

        # 3ë‹¨ê³„: ì ìˆ˜ ì •ê·œí™” ë° í•˜ì´ë¸Œë¦¬ë“œ ê³„ì‚°
        def normalize_scores(scores):
            """ì ìˆ˜ë¥¼ 0~1 ë²”ìœ„ë¡œ ì •ê·œí™”"""
            if not scores or max(scores) == min(scores):
                return [0.5] * len(scores)  # ëª¨ë“  ì ìˆ˜ê°€ ê°™ìœ¼ë©´ ì¤‘ê°„ê°’
            
            min_score, max_score = min(scores), max(scores)
            return [(s - min_score) / (max_score - min_score) for s in scores]

        # Dense ì ìˆ˜ ì •ê·œí™” (ê±°ë¦¬ â†’ ìœ ì‚¬ë„ë¡œ ë³€í™˜ í›„)
        # ChromaDBì˜ cosine distanceë¥¼ similarityë¡œ ë³€í™˜: similarity = 1 - distance
        dense_similarities = [1 - score for score in dense_raw_scores]
        dense_normalized = normalize_scores(dense_similarities)
        
        # Sparse ì ìˆ˜ ì •ê·œí™”
        sparse_normalized = normalize_scores(sparse_raw_scores)
        
        # âœ… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        hybrid_results = []
        for i, (doc, dense_norm, sparse_norm, dense_raw, sparse_raw) in enumerate(
            zip(dense_docs, dense_normalized, sparse_normalized, dense_raw_scores, sparse_raw_scores)
        ):
            # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: Dense 60% + Sparse 40%
            final_score = dense_norm * 0.6 + sparse_norm * 0.4
            
            hybrid_results.append((doc, final_score, dense_norm, sparse_norm))
            
            print(f"[DEBUG][Rank {i+1}] Name:{doc.metadata.get('ìº í•‘ì¥ì´ë¦„', 'ì´ë¦„ì—†ìŒ')} "
                  f"Dense:{dense_norm:.3f}({dense_raw:.3f}) "
                  f"Sparse:{sparse_norm:.3f}({sparse_raw:.4f}) "
                  f"Final:{final_score:.3f}")

        # 4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì¬ì •ë ¬ â†’ ìƒìœ„ 2ê°œ ì„ íƒ
        hybrid_results.sort(key=lambda x: x[1], reverse=True)
        top_2_results = hybrid_results[:2]
        
        print(f"ğŸ”¹ [DEBUG] ìµœì¢… ìƒìœ„ 2ê°œ ì„ íƒ:")
        for i, (doc, final_score, dense_score, sparse_score) in enumerate(top_2_results):
            print(f"[DEBUG][Final Top {i+1}] Name:{doc.metadata.get('ìº í•‘ì¥ì´ë¦„', 'ì´ë¦„ì—†ìŒ')} "
                  f"Score:{final_score:.3f}")

        # 5ë‹¨ê³„: ì›¹ ìŠ¤ë‹ˆí« ì¶”ê°€ (ê¸°ì¡´ê³¼ ë™ì¼)
        web_snippets_map = {}
        if camping_type != "ì˜¤ì§€/ë…¸ì§€ìº í•‘" and top_2_results:
            docs_with_metadata = [(doc, score) for doc, score, _, _ in top_2_results]
            
            snippets = await build_snippet_per_doc(
                docs_with_metadata=docs_with_metadata,
                per_type_display=20,
                fetch_timeout=8,
                max_chars=2000,
                enforce_name_in_title=True,
                include_when_no_local_modified=True,
            )
            web_snippets_map = {s['ì¥ì†Œì´ë¦„']: s for s in snippets}

        # 6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë°˜í™˜
        locations_data = []
        unique_names = set()

        for doc, final_score, _, _ in top_2_results:
            meta = getattr(doc, "metadata", {}) or {}
            location_name = meta.get("ìº í•‘ì¥ì´ë¦„", "ì´ë¦„ ì •ë³´ ì—†ìŒ")
            
            if location_name in unique_names:
                continue
            unique_names.add(location_name)

            location_info = {
                "local_document": {
                    "metadata": meta, 
                    "content": doc.page_content, 
                    "score": final_score
                },
                "web_snippet": web_snippets_map.get(location_name)
            }
            locations_data.append(location_info)

        return {"locations": locations_data}

    except Exception as e:
        print(f"âŒ [ERROR] search_camping ì „ì²´ ì‹¤íŒ¨: {e}")
        return {"locations": []}

        
async def search_paid_camping(state):
    return await search_camping(state, "ìœ ë£Œìº í•‘ì¥", vectordb)

async def search_glamping_caravan(state):
    return await search_camping(state, "ê¸€ë¨í•‘/ì¹´ë¼ë°˜", vectordb)

async def search_ojee_camping(state):
    return await search_camping(state, "ì˜¤ì§€/ë…¸ì§€ìº í•‘", vectordb)

# --- ì¥ì†Œ ì¶”ì²œ ìµœì¢… ë‹µë³€ ìƒì„± ---
def generate_location_answer(state: GraphState) -> dict:
    t0 = time.perf_counter()
    
    original_question = state.get("original_question", "")
    second_question = state.get("question", "")
    camping_type = state.get("camping_type_preference", "")
    locations = state.get("locations", [])
    print(f"[DEBUG] ìµœì¢… ë°˜í™˜ë  ì¥ì†Œ ê°œìˆ˜: {len(locations)}ê°œ")
    
    context_strs = []
    final_locations = []
    for loc in locations:
        local_meta = loc.get("local_document", {}).get("metadata", {})
        local_content = loc.get("local_document", {}).get("content", "")
        snippet = loc.get("web_snippet", {})
        snippet_text = snippet.get("snippet", "") if snippet else ""
        
        # ì˜¬ë°”ë¥¸ ë©”íƒ€ë°ì´í„° ì ‘ê·¼ ë° final_locations ìƒì„±
        location_data = {
            "name": local_meta.get('ìº í•‘ì¥ì´ë¦„'),
            "address": local_meta.get('ìº í•‘ì¥ì£¼ì†Œ'),
            "latitude": local_meta.get('ìº í•‘ì¥ ìœ„ë„'),
            "longitude": local_meta.get('ìº í•‘ì¥ ê²½ë„'),
            "local_meta": local_meta,
        }
        final_locations.append(location_data)
        
        context_strs.append(
            f"ë©”íƒ€ë°ì´í„°: {final_locations}"
            f"ë³¸ë¬¸: {local_content}"
            f"ë„¤ì´ë²„ ì •ë³´: {snippet_text}"
        )

    context_str = "\n---\n".join(context_strs[:2])

    prompt = (
        f"ë‹¹ì‹ ì€ ìº í•‘ì—ì´ì „íŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ '{camping_type}' ìœ í˜•ì— ë§ëŠ” ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n"
        "ì¶”ì²œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ ë‚´ ë©”íƒ€ë°ì´í„°ì™€ ìµœì‹  ë„¤ì´ë²„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì‚¬ì´íŠ¸ë‚˜ ì „í™”ë²ˆí˜¸ë¥¼ ë§í•´ì¤„ë•ŒëŠ” \"ëª¨ë“  ì •ë³´ëŠ” ìµœì‹ ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸/ì „í™”ë¡œ ì¬í™•ì¸ ë°”ëë‹ˆë‹¤.\"ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.\n\n"
        
        f"ì²« ë²ˆì§¸ ì§ˆë¬¸: {original_question}\n"
        f"ë‘ ë²ˆì§¸ ì§ˆë¬¸ ë° ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ë‹µë³€: {second_question}\n\n"
        
        f"ë¬¸ë§¥:\n{context_str}\n\n"
        
        "ìœ„ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        "ë‹µë³€:"
    )
    try:
        out = oai_text(prompt)
        answer = out["text"]
        print(f"âœ… ì¥ì†Œ ì¶”ì²œ ë‹µë³€ ìƒì„± ì™„ë£Œ ({time.perf_counter() - t0:.2f}s | req={out['request_id']}) ")
        # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ë‹¬í•  ìµœì¢… locations í¬ë§·ìœ¼ë¡œ ë°˜í™˜
        return {"final_answer": answer, "locations": final_locations}
    except Exception as e:
        print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e), "locations": final_locations}


# --- ë©”ì¸ ì›Œí¬í”Œë¡œìš° ---
workflow = StateGraph(GraphState)
workflow.add_node("classify_type", classify_question_type)
workflow.add_node("generate_general", generate_general_answer)
workflow.add_node("ask_preference", ask_camping_preference)
workflow.set_entry_point("classify_type")
workflow.add_conditional_edges(
    "classify_type",
    route_by_classification,
    {
        "general": "generate_general",
        "ask_preference": "ask_preference",
    },
)
workflow.add_edge("generate_general", END)
workflow.add_edge("ask_preference", END)

# --- ìº í•‘ ìœ í˜• í›„ì† ì›Œí¬í”Œë¡œìš° ---
continuation = StateGraph(GraphState)
continuation.add_node("classify_camping", classify_camping_type)
continuation.add_node("search_paid", search_paid_camping)
continuation.add_node("search_glamping", search_glamping_caravan)
continuation.add_node("search_ojee", search_ojee_camping)
continuation.add_node("generate_location", generate_location_answer)
continuation.set_entry_point("classify_camping")
continuation.add_conditional_edges(
    "classify_camping",
    route_by_camping_type,
    {
        "paid": "search_paid",
        "glamping": "search_glamping",
        "ojee": "search_ojee",
    },
)
continuation.add_edge("search_paid", "generate_location")
continuation.add_edge("search_glamping", "generate_location")
continuation.add_edge("search_ojee", "generate_location")
continuation.add_edge("generate_location", END)

main_app = workflow.compile()
continuation_app = continuation.compile()

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---

def main():
    print("ğŸ•ï¸ ìº í•‘ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    waiting_for_camping_choice = False
    original_q_cache: str | None = None

    while True:
        user_input = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if user_input.lower() in ["ì¢…ë£Œ", "quit", "exit"]:
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        try:
            if waiting_for_camping_choice:
                # ìº í•‘ ìœ í˜• ì…ë ¥ ëŒ€ê¸° ì¤‘
                state: GraphState = {
                    "question": user_input,
                    "original_question": original_q_cache or user_input,
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                }
                result = asyncio.run(continuation_app.ainvoke(state))
                waiting_for_camping_choice = False
                original_q_cache = None
            else:
                # ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬
                state: GraphState = {
                    "question": user_input,
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                }
                result = asyncio.run(main_app.ainvoke(state))

                # ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ìœ í˜•ì„ ë°›ë„ë¡ ì „í™˜
                if "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?" in result.get("final_answer", ""):
                    waiting_for_camping_choice = True
                    original_q_cache = state["question"]

            print(f"\nğŸ“ ë‹µë³€: {result.get('final_answer')}\n")

        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            waiting_for_camping_choice = False
            original_q_cache = None

if __name__ == "__main__":
    main()