import os
import time
import asyncio
import random
import operator
from typing import TypedDict, Annotated, List, Any, Dict

# OpenAI SDK
from openai import OpenAI

# LangChain / Vector store
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv

load_dotenv()

# ===== í™˜ê²½ ì„¤ì • =====
# âš ï¸ í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì½ìŠµë‹ˆë‹¤: export OPENAI_API_KEY="sk-..."
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=30.0,
)

EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large-instruct"
CHROMA_DB_PATH = "./data/camp_chroma_store"
MAX_LOOPS = 2

GPT_MODEL = "ft:gpt-4.1-2025-04-14:ailab::C2eTINXG:ckpt-step-357"

# ===== ìƒíƒœ ì •ì˜ =====
class GraphState(TypedDict):
    question: str
    original_question: str
    classification: str
    camping_type_preference: str
    context: Annotated[List[Any], operator.add]
    locations: Annotated[List[dict], operator.add]
    final_answer: str
    loop_count: int
    search_attempted: bool
    error_message: str

# ===== ê³µìš© LLM í˜¸ì¶œ ìœ í‹¸ =====

def oai_text(prompt: str, model: str = GPT_MODEL) -> Dict[str, Any]:
    """Responses APIë¡œ í…ìŠ¤íŠ¸ë§Œ ë°›ì•„ì˜¤ëŠ” í—¬í¼.
    ë°˜í™˜: {"text": str, "request_id": str}
    """
    resp = client.responses.create(
        model=model,
        input=prompt,
    )
    # ê³µì‹ SDKëŠ” output_text ì œê³µ â†’ íŒŒì‹± ì¸ë±ìŠ¤ ì‹¤ìˆ˜ ë°©ì§€
    return {"text": (resp.output_text or "").strip(), "request_id": getattr(resp, "_request_id", None)}

# ===== ì„ë² ë”© ë° DB ë¡œë”© =====
print("--- ì„ë² ë”© ëª¨ë¸ ë° ChromaDB ë¡œë”© ì¤‘ ---")
start_time = time.perf_counter()
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
vectordb = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
end_time = time.perf_counter()
print(f"--- ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")

# --- 1ì°¨ ë¶„ë¥˜ ë…¸ë“œ ---

def classify_question_type(state: GraphState) -> dict:
    t0 = time.perf_counter()
    question = state["question"]
    prompt = (
        "ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  'ì¼ë°˜ ìº í•‘' ë˜ëŠ” 'ì¥ì†Œ ì¶”ì²œ' ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n"
        "ë¶„ë¥˜:"
    )
    try:
        out = oai_text(prompt)
        classification = (out["text"].splitlines() or [""])[0].strip()
        if classification not in ["ì¼ë°˜ ìº í•‘", "ì¥ì†Œ ì¶”ì²œ"]:
            classification = "ì¼ë°˜ ìº í•‘"
        print(f"ë¶„ë¥˜: {classification} ({time.perf_counter() - t0:.2f}s)")
        return {"classification": classification, "original_question": question}
    except Exception as e:
        print(f"ë¶„ë¥˜ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"classification": "ì¼ë°˜ ìº í•‘", "original_question": question, "error_message": str(e)}

# --- ì¼ë°˜ ì§ˆë¬¸ ë‹µë³€ ë…¸ë“œ ---

def generate_general_answer(state: GraphState) -> dict:
    t0 = time.perf_counter()
    question = state["question"]
    prompt = (
        "ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìº í•‘ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n"
        "ë‹µë³€:"
    )
    try:
        out = oai_text(prompt)
        answer = out["text"]
        print(f"ì¼ë°˜ì§ˆë¬¸ ì‘ë‹µ ({time.perf_counter() - t0:.2f}s | req={out['request_id']})")
        return {"final_answer": answer}
    except Exception as e:
        print(f"ì¼ë°˜ì§ˆë¬¸ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e)}

# --- ë¼ìš°íŒ… í•¨ìˆ˜ ---

def route_by_classification(state):
    return "general" if state.get("classification") == "ì¼ë°˜ ìº í•‘" else "ask_preference"


def route_by_camping_type(state):
    mapping = {
        "ìœ ë£Œìº í•‘ì¥": "paid",
        "ê¸€ë¨í•‘/ì¹´ë¼ë°˜": "glamping",
        "ì˜¤ì§€/ë…¸ì§€ìº í•‘": "ojee",
    }
    return mapping.get(state.get("camping_type_preference", "ìœ ë£Œìº í•‘ì¥"), "paid")

# --- ìº í•‘ ìœ í˜• ì„ íƒ ìš”ì²­ ë…¸ë“œ ---

def ask_camping_preference(state: GraphState) -> dict:
    message = (
        "ğŸ•ï¸ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”! ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?\n\n"
        "1ï¸âƒ£ ìœ ë£Œìº í•‘ì¥ (ì˜¤í† ìº í•‘ì¥, í¸ì˜ì‹œì„¤ ì™„ë¹„)  \n"
        "2ï¸âƒ£ ê¸€ë¨í•‘/ì¹´ë¼ë°˜ (ëŸ­ì…”ë¦¬, í¸ì•ˆí•œ ìº í•‘)  \n"
        "3ï¸âƒ£ ì˜¤ì§€/ë…¸ì§€ìº í•‘ (ìì—° ì† ë¬´ë£Œ ìº í•‘)  \n\n"
        "ì¸ì›ì´ë‚˜ ìŠ¤íƒ€ì¼ ë“± ëª¨ë‘ ì•Œë ¤ì£¼ì„¸ìš”!"
    )
    return {"final_answer": message}

# --- ìº í•‘ ìœ í˜• ë¶„ë¥˜ ë…¸ë“œ ---

def classify_camping_type(state: GraphState) -> dict:
    t0 = time.perf_counter()
    user_input = state["question"]  # ë‘ ë²ˆì§¸ ì§ˆë¬¸, ìº í•‘ ìœ í˜•ì— ëŒ€í•œ ë‹µë³€
    original_question = state.get("original_question", "")
    prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ì„ í˜¸ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìì˜ ì‘ë‹µê³¼ ì›ë˜ ì§ˆë¬¸ì„ ì½ê³  'ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘/ì¹´ë¼ë°˜', 'ì˜¤ì§€/ë…¸ì§€ìº í•‘' ì¤‘ í•˜ë‚˜ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì›ë˜ ì§ˆë¬¸: {original_question}\n"
        f"ì‚¬ìš©ì ì‘ë‹µ: {user_input}\n"
        "ë¶„ë¥˜:"
    )
    try:
        out = oai_text(prompt)
        camping_type = (out["text"].splitlines() or [""])[0].strip()
        if camping_type not in ["ìœ ë£Œìº í•‘ì¥", "ê¸€ë¨í•‘/ì¹´ë¼ë°˜", "ì˜¤ì§€/ë…¸ì§€ìº í•‘"]:
            camping_type = "ìœ ë£Œìº í•‘ì¥"
        print(f"ìº í•‘ìœ í˜•: {camping_type} ({time.perf_counter() - t0:.2f}s)")
        return {"camping_type_preference": camping_type}
    except Exception as e:
        print(f"ìº í•‘ìœ í˜• ë¶„ë¥˜ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"camping_type_preference": "ìœ ë£Œìº í•‘ì¥", "error_message": str(e)}

# --- RAG + ì›¹ê²€ìƒ‰ ë…¸ë“œ (ì›¹ê²€ìƒ‰ì€ mock) ---

async def general_web_search_async(query: str, camping_type: str) -> List[str]:
    await asyncio.sleep(0.2)  # ì‹¤ì œ API ëŒ€ê¸° ì‹œë®¬ë ˆì´ì…˜
    mock_results = {
        "ìœ ë£Œìº í•‘ì¥": [
            "[ì›¹] ì†ì´ˆ ì˜¤í† ìº í•‘ì¥ í›„ê¸°",
            "[ì›¹] ê°•ë¦‰ í•´ë³€ ìº í•‘ì¥ ì¶”ì²œ",
            "[ì›¹] ì–‘ì–‘ ìº í•‘ì¥ ì˜ˆì•½ íŒ",
        ],
        "ê¸€ë¨í•‘/ì¹´ë¼ë°˜": [
            "[ì›¹] ê°€í‰ ì¹´ë¼ë°˜ ìˆ™ì†Œ ë¦¬ë·°",
            "[ì›¹] ì œì£¼ ê¸€ë¨í•‘ ì¸ê¸° ì¥ì†Œ",
            "[ì›¹] ë‚¨í•´ ê¸€ë¨í•‘ ì‹œì„¤ ì•ˆë‚´",
        ],
        "ì˜¤ì§€/ë…¸ì§€ìº í•‘": [
            "[ì›¹] ì œì£¼ë„ ë…¸ì§€ìº í•‘ ìŠ¤íŒŸ",
            "[ì›¹] ê°•ì›ë„ ì˜¤ì§€ìº í•‘ ê¸ˆì§€ êµ¬ì—­",
            "[ì›¹] ì°¨ë°• ì„±ì§€ ë² ìŠ¤íŠ¸ 5",
        ],
    }
    pool = mock_results.get(camping_type, [])
    k = min(3, len(pool))
    return random.sample(pool, k=k) if k > 0 else []


async def search_camping(state: GraphState, camping_type: str) -> dict:
    question = state.get("original_question", state["question"])
    print(f"\n--- ğŸ” {camping_type} ìœ í˜•ìœ¼ë¡œ ë²¡í„°DB ê²€ìƒ‰ ì‹œì‘ ---")
    try:
        # ë²¡í„°DBì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œ ë©”íƒ€í•„í„° ì§€ì› ìœ ë¬´ì— ë”°ë¼ ë¶„ê¸°
        try:
            docs_with_metadata = await asyncio.to_thread(
                vectordb.similarity_search_with_score,
                query=question,
                k=2,
                filter={"ìº í•‘ìœ í˜•": camping_type},
            )
        except TypeError:
            # langchain/chroma ë²„ì „ì— ë”°ë¼ filter ì¸ì ë¯¸ì§€ì› ê°€ëŠ¥ â†’ í•„í„° ì—†ì´ ê²€ìƒ‰
            docs_with_metadata = await asyncio.to_thread(
                vectordb.similarity_search_with_score,
                query=question,
                k=2,
            )

        context: List[str] = []
        locations: List[dict] = []
        unique_names = set()
        if docs_with_metadata:
            print("âœ”ï¸ RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            for i, (doc, score) in enumerate(docs_with_metadata):
                location_name = doc.metadata.get("ìº í•‘ì¥ì´ë¦„", "ì´ë¦„ ì •ë³´ ì—†ìŒ")
                if location_name not in unique_names:
                    unique_names.add(location_name)
                    
                    metadata_str = f"ë©”íƒ€ë°ì´í„°: {getattr(doc, 'metadata', {})}"
                    content_with_metadata = f"ë¬¸ì„œ ë‚´ìš©: {doc.page_content}\n{metadata_str}"
                    context.append(content_with_metadata)
                    
                    location_info = {
                        "name": location_name,
                        "address": doc.metadata.get("ìº í•‘ì¥ì£¼ì†Œ", "ì£¼ì†Œ ì •ë³´ ì—†ìŒ"),
                        "latitude": doc.metadata.get("ìœ„ë„", None),
                        "longitude": doc.metadata.get("ê²½ë„", None)
                    }
                    locations.append(location_info)
                    
                    print(
                        f"  [{i+1}] ë¬¸ì„œ: {doc.page_content[:40]}... | ìœ ì‚¬ë„: {score:.4f} | ë©”íƒ€ë°ì´í„°: {getattr(doc, 'metadata', {})}"
                    )
        else:
            print(f"âš ï¸ {camping_type} ìœ í˜•ì— ëŒ€í•œ ë¬¸ì„œê°€ ë²¡í„°DBì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # ì˜¤ì§€/ë…¸ì§€ìº í•‘ì€ ì›¹ ê²€ìƒ‰ ìƒëµ
        if camping_type != "ì˜¤ì§€/ë…¸ì§€ìº í•‘":
            print("--- ğŸŒ ì›¹ ê²€ìƒ‰ ì‹œì‘ ---")
            web_results = await general_web_search_async(question, camping_type)
            if web_results:
                print("âœ”ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                for i, res in enumerate(web_results):
                    context.append(res)
                    print(f"  [ì›¹{i+1}] {res}")
            else:
                print("âš ï¸ ì›¹ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print("ğŸŒ 'ì˜¤ì§€/ë…¸ì§€ìº í•‘' ìœ í˜•ì€ ì›¹ ê²€ìƒ‰ì„ ìƒëµí•©ë‹ˆë‹¤.")

        print(f"[DEBUG] search_camping í•¨ìˆ˜ì—ì„œ ë°˜í™˜ë  locations: {locations}")
        return {"context": context, "locations": locations, "search_attempted": True}
    
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"context": ["ê²€ìƒ‰ ì˜¤ë¥˜ ë°œìƒ"], "locations": [], "search_attempted": True, "error_message": str(e)}


async def search_paid_camping(state):
    return await search_camping(state, "ìœ ë£Œìº í•‘ì¥")


async def search_glamping_caravan(state):
    return await search_camping(state, "ê¸€ë¨í•‘/ì¹´ë¼ë°˜")


async def search_ojee_camping(state):
    return await search_camping(state, "ì˜¤ì§€/ë…¸ì§€ìº í•‘")


# --- ì¥ì†Œ ì¶”ì²œ ìµœì¢… ë‹µë³€ ìƒì„± ---

def generate_location_answer(state: GraphState) -> dict:
    t0 = time.perf_counter()
    
    original_question = state.get("original_question", "")
    second_question = state.get("question", "")
    camping_type = state.get("camping_type_preference", "")
    context = state.get("context", [])
    locations = state.get("locations", [])
    print(f"[DEBUG] ìµœì¢… ë°˜í™˜ë  ì¥ì†Œ ê°œìˆ˜: {len(locations)}ê°œ")
    
    context_str = "\n".join(str(ctx) for ctx in context[:5])

    prompt = (
        f"ë‹¹ì‹ ì€ ìº í•‘ì—ì´ì „íŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ '{camping_type}' ìœ í˜•ì— ë§ëŠ” ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.\n"
        "ì¶”ì²œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ ë‚´ ë©”íƒ€ë°ì´í„°ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì‚¬ì´íŠ¸ë‚˜ ì „í™”ë²ˆí˜¸ë¥¼ ë§í•´ì¤„ë•ŒëŠ” \"ëª¨ë“  ì •ë³´ëŠ” ìµœì‹ ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸/ì „í™”ë¡œ ì¬í™•ì¸ ë°”ëë‹ˆë‹¤.\"ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.\n\n"
        
        f"ì²« ë²ˆì§¸ ì§ˆë¬¸: {original_question}\n"
        f"ë‘ ë²ˆì§¸ ì§ˆë¬¸ ë° ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ë‹µë³€: {second_question}\n\n"
        
        f"ë¬¸ë§¥:\n{context_str}\n\n"
        
        "ìœ„ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        "ë‹µë³€:"
    )
    try:
        out = oai_text(prompt)
        answer = out["text"]
        print(f"âœ… ì¥ì†Œ ì¶”ì²œ ë‹µë³€ ìƒì„± ì™„ë£Œ ({time.perf_counter() - t0:.2f}s | req={out['request_id']})")
        return {"final_answer": answer, "locations": locations}
    except Exception as e:
        print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e), "locations": locations}


# --- ë©”ì¸ ì›Œí¬í”Œë¡œìš° ---
workflow = StateGraph(GraphState)
workflow.add_node("classify_type", classify_question_type)
workflow.add_node("generate_general", generate_general_answer)
workflow.add_node("ask_preference", ask_camping_preference)
workflow.set_entry_point("classify_type")
workflow.add_conditional_edges(
    "classify_type",
    route_by_classification,
    {
        "general": "generate_general",
        "ask_preference": "ask_preference",
    },
)
workflow.add_edge("generate_general", END)
workflow.add_edge("ask_preference", END)

# --- ìº í•‘ ìœ í˜• í›„ì† ì›Œí¬í”Œë¡œìš° ---
continuation = StateGraph(GraphState)
continuation.add_node("classify_camping", classify_camping_type)
continuation.add_node("search_paid", search_paid_camping)
continuation.add_node("search_glamping", search_glamping_caravan)
continuation.add_node("search_ojee", search_ojee_camping)
continuation.add_node("generate_location", generate_location_answer)
continuation.set_entry_point("classify_camping")
continuation.add_conditional_edges(
    "classify_camping",
    route_by_camping_type,
    {
        "paid": "search_paid",
        "glamping": "search_glamping",
        "ojee": "search_ojee",
    },
)
continuation.add_edge("search_paid", "generate_location")
continuation.add_edge("search_glamping", "generate_location")
continuation.add_edge("search_ojee", "generate_location")
continuation.add_edge("generate_location", END)

main_app = workflow.compile()
continuation_app = continuation.compile()

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---

def main():
    print("ğŸ•ï¸ ìº í•‘ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    waiting_for_camping_choice = False
    original_q_cache: str | None = None

    while True:
        user_input = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if user_input.lower() in ["ì¢…ë£Œ", "quit", "exit"]:
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        try:
            if waiting_for_camping_choice:
                # ìº í•‘ ìœ í˜• ì…ë ¥ ëŒ€ê¸° ì¤‘
                state: GraphState = {
                    "question": user_input,
                    "original_question": original_q_cache or user_input,
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                }
                result = asyncio.run(continuation_app.ainvoke(state))
                waiting_for_camping_choice = False
                original_q_cache = None
            else:
                # ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬
                state: GraphState = {
                    "question": user_input,
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                }
                result = asyncio.run(main_app.ainvoke(state))

                # ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ìœ í˜•ì„ ë°›ë„ë¡ ì „í™˜
                if "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?" in result.get("final_answer", ""):
                    waiting_for_camping_choice = True
                    original_q_cache = state["question"]

            print(f"\nğŸ“ ë‹µë³€: {result.get('final_answer')}\n")

        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            waiting_for_camping_choice = False
            original_q_cache = None


if __name__ == "__main__":
    main()
