import os
import time
import asyncio
import random
import operator
from typing import TypedDict, Annotated, List, Any, Dict

#naver api
from naver_api import build_snippet_per_doc, format_snippets_as_text

# OpenAI SDK
from openai import OpenAI

# LangChain / Vector store
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langgraph.graph import StateGraph, END
from langchain_core.documents import Document
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
import numpy as np
from langchain.schema import Document
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.embeddings.base import Embeddings

# BGE-M3 í†µí•© ì‚¬ìš©
from FlagEmbedding import BGEM3FlagModel

load_dotenv()

# ===== í™˜ê²½ ì„¤ì • =====
# âš ï¸ í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì½ìŠµë‹ˆë‹¤: export OPENAI_API_KEY="sk-..."
client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    timeout=30.0,
)

EMBEDDING_MODEL_NAME = "dragonkue/BGE-m3-ko"
CHROMA_DB_PATH = "./data/camp_vectorDB_BGE_sentence_per_doc"
MAX_LOOPS = 2

GPT_MODEL = "ft:gpt-4.1-mini-2025-04-14:ailab:camping-rag-qa:C2bHhwJM:ckpt-step-714"

# âœ… ìƒˆë¡œìš´ í†µí•© BGE-M3 í´ë˜ìŠ¤
class UnifiedBGEM3Embedder:
    def __init__(self, model_name="dragonkue/BGE-m3-ko"):
        self.model = None
        self.model_name = model_name
        self._load_model()
    
    def _load_model(self):
        """BGE-M3 ëª¨ë¸ ë¡œë“œ"""
        try:
            self.model = BGEM3FlagModel(self.model_name, use_fp16=True)
            print(f"âœ… BGE-M3 í†µí•© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")
        except Exception as e:
            print(f"âŒ BGE-M3 í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            try:
                # Fallback: ì›ë³¸ BGE-M3 ëª¨ë¸
                self.model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
                print("âœ… BGE-M3 ì›ë³¸ ëª¨ë¸ë¡œ Fallback ë¡œë“œ ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ BGE-M3 Fallbackë„ ì‹¤íŒ¨: {e2}")
                self.model = None
    
    def encode_for_vector_db(self, texts, batch_size=12):
        """ChromaDBìš© dense embedding ìƒì„±"""
        if self.model is None:
            raise Exception("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        
        embeddings = self.model.encode(
            texts,
            batch_size=batch_size,
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False
        )
        return embeddings['dense_vecs']
    
    def encode_hybrid(self, query_text, candidate_texts, batch_size=12):
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© dense + sparse ë™ì‹œ ê³„ì‚°"""
        if self.model is None:
            raise Exception("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•ŠìŒ")
        
        all_texts = [query_text] + candidate_texts
        embeddings = self.model.encode(
            all_texts,
            batch_size=batch_size,
            return_dense=True,
            return_sparse=True,
            return_colbert_vecs=False
        )
        
        return {
            'query_dense': embeddings['dense_vecs'][0],
            'doc_dense': embeddings['dense_vecs'][1:],
            'query_sparse': embeddings['lexical_weights'][0],
            'doc_sparse': embeddings['lexical_weights'][1:]
        }

# âœ… ChromaDBìš© ì»¤ìŠ¤í…€ ì„ë² ë”© í•¨ìˆ˜
class BGEM3LangChainEmbeddings(Embeddings):
    def __init__(self, unified_embedder):
        self.unified_embedder = unified_embedder
    
    def embed_documents(self, texts):
        """ë¬¸ì„œë“¤ ì„ë² ë”©"""
        try:
            embeddings = self.unified_embedder.encode_for_vector_db(texts)
            return embeddings.tolist()
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            # Fallback: ë¹ˆ ë²¡í„° ë°˜í™˜
            return [[0.0] * 1024 for _ in texts]
    
    def embed_query(self, text):
        """ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©"""
        try:
            embeddings = self.unified_embedder.encode_for_vector_db([text])
            return embeddings[0].tolist()
        except Exception as e:
            print(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            # Fallback: ë¹ˆ ë²¡í„° ë°˜í™˜
            return [0.0] * 1024

# âœ… í†µí•© ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
unified_embedder = UnifiedBGEM3Embedder(EMBEDDING_MODEL_NAME)

# ===== ìƒíƒœ ì •ì˜ =====
class GraphState(TypedDict):
    question: str
    original_question: str
    classification: str
    camping_type_preference: str
    context: Annotated[List[Any], operator.add]
    locations: Annotated[List[dict], operator.add]
    final_answer: str
    loop_count: int
    search_attempted: bool
    error_message: str

# ===== ê³µìš© LLM í˜¸ì¶œ ìœ í‹¸ =====

def oai_text(prompt: str, model: str = GPT_MODEL) -> Dict[str, Any]:
    """Responses APIë¡œ í…ìŠ¤íŠ¸ë§Œ ë°›ì•„ì˜¤ëŠ” í—¬í¼.
    ë°˜í™˜: {"text": str, "request_id": str}
    """
    resp = client.responses.create(
        model=model,
        input=prompt,
    )
    # ê³µì‹ SDKëŠ” output_text ì œê³µ â†’ íŒŒì‹± ì¸ë±ìŠ¤ ì‹¤ìˆ˜ ë°©ì§€
    return {"text": (resp.output_text or "").strip(), "request_id": getattr(resp, "_request_id", None)}

# ===== ì„ë² ë”© ë° DB ë¡œë”© =====
print("--- ChromaDB ë¡œë”© ì¤‘ ---")
start_time = time.perf_counter()

# âœ… ë³€ê²½: í†µí•© ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©
embedding_function = BGEM3LangChainEmbeddings(unified_embedder)
vectordb = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embedding_function)

end_time = time.perf_counter()
print(f"--- ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")

# --- 1ì°¨ ë¶„ë¥˜ ë…¸ë“œ ---

def classify_question_type(state: GraphState) -> dict:
    t0 = time.perf_counter()
    question = state["question"]
    prompt = (
        "ë‹¹ì‹ ì€ ìº í•‘ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë†€ëŸ¬ê°ˆ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë‹¬ë¼ê³  í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  ì¼ë°˜ ìº í•‘ì— ê´€í•œ ì§ˆë¬¸ì´ë©´ 'ì¼ë°˜ ìº í•‘' ìœ¼ë¡œ ë‹µí•˜ê³ , ë†€ëŸ¬ê°€ëŠ” ì¥ì†Œì— ê´€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ë§¥ìƒ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì€ 'ì¥ì†Œ ì¶”ì²œ'ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n" 
        "'ì¼ë°˜ ìº í•‘', 'ì¥ì†Œ ì¶”ì²œ' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n"
        "ë¶„ë¥˜:"
    )
    try:
        out = oai_text(prompt)
        text = out["text"]

        categories = ["ì¼ë°˜ ìº í•‘", "ì¥ì†Œ ì¶”ì²œ"]

        # ë¬¸ì¥ ì „ì²´ì—ì„œ ì¹´í…Œê³ ë¦¬ ë‹¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸
        classification = next((cat for cat in categories if cat in text), "ì¼ë°˜ ìº í•‘")

        print(f"ë¶„ë¥˜: {classification} ({time.perf_counter() - t0:.2f}s)")
        return {"classification": classification, "original_question": question}

    except Exception as e:
        print(f"ë¶„ë¥˜ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"classification": "ì¼ë°˜ ìº í•‘", "original_question": question, "error_message": str(e)}
        
# --- ì¼ë°˜ ì§ˆë¬¸ ë‹µë³€ ë…¸ë“œ ---

def generate_general_answer(state: GraphState) -> dict:
    t0 = time.perf_counter()
    question = state["question"]
    prompt = (
        "ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìº í•‘ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n"
        "ì¤„ë°”ê¿ˆì„ ì´ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."
        "ë‹µë³€:"
    )
    try:
        out = oai_text(prompt)
        answer = out["text"]
        print(f"ì¼ë°˜ì§ˆë¬¸ ì‘ë‹µ ({time.perf_counter() - t0:.2f}s | req={out['request_id']})")
        return {"final_answer": answer}
    except Exception as e:
        print(f"ì¼ë°˜ì§ˆë¬¸ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e)}

# --- ë¼ìš°íŒ… í•¨ìˆ˜ ---

def route_by_classification(state):
    return "general" if state.get("classification") == "ì¼ë°˜ ìº í•‘" else "ask_preference"

def route_by_camping_type(state):
    mapping = {
        "ìœ ë£Œìº í•‘ì¥": "paid",
        "ê¸€ë¨í•‘/ì¹´ë¼ë°˜": "glamping",
        "ì˜¤ì§€/ë…¸ì§€ìº í•‘": "ojee",
    }
    return mapping.get(state.get("camping_type_preference", "ìœ ë£Œìº í•‘ì¥"), "paid")

# --- ìº í•‘ ìœ í˜• ì„ íƒ ìš”ì²­ ë…¸ë“œ ---

def ask_camping_preference(state: GraphState) -> dict:
    message = (
        "ğŸ•ï¸ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”! ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?\n\n"
        "â— ìœ ë£Œìº í•‘ì¥ (ì˜¤í† ìº í•‘ì¥, í¸ì˜ì‹œì„¤ ì™„ë¹„)\n"
        "â— ê¸€ë¨í•‘/ì¹´ë¼ë°˜ (ëŸ­ì…”ë¦¬, í¸ì•ˆí•œ ìº í•‘)\n"
        "â— ì˜¤ì§€/ë…¸ì§€ìº í•‘ (ìì—° ì† ë¬´ë£Œ ìº í•‘)\n\n"
        "ì¸ì›ì´ë‚˜ ìŠ¤íƒ€ì¼ ë“± ëª¨ë‘ ì•Œë ¤ì£¼ì„¸ìš”!"
    )
    return {"final_answer": message}

# --- ìº í•‘ ìœ í˜• ë¶„ë¥˜ ë…¸ë“œ ---
def classify_camping_type(state: GraphState) -> dict:
    t0 = time.perf_counter()
    user_input = state["question"]  # ì‚¬ìš©ìì˜ ë‹µë³€
    original_question = state.get("original_question", "")
    
    prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ì„ í˜¸ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìì˜ ì‘ë‹µê³¼ ì›ë˜ ì§ˆë¬¸ì„ ì½ê³  'ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘/ì¹´ë¼ë°˜', 'ì˜¤ì§€/ë…¸ì§€ìº í•‘' ì¤‘ í•˜ë‚˜ë¡œ ì •í™•í•˜ê²Œ ì¹´í…Œê³ ë¦¬ë§Œì„ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì›ë˜ ì§ˆë¬¸: {original_question}\n"
        f"ì‚¬ìš©ì ì‘ë‹µ: {user_input}\n"
        "ë¶„ë¥˜:"
    )
    
    try:
        out = oai_text(prompt)
        text = out["text"]

        categories = ["ìœ ë£Œìº í•‘ì¥", "ê¸€ë¨í•‘/ì¹´ë¼ë°˜", "ì˜¤ì§€/ë…¸ì§€ìº í•‘"]
        camping_type = next((cat for cat in categories if cat in text), "ìœ ë£Œìº í•‘ì¥")

        print(f"ìº í•‘ìœ í˜•: {camping_type} ({time.perf_counter() - t0:.2f}s)")
        return {"camping_type_preference": camping_type}

    except Exception as e:
        print(f"ìº í•‘ìœ í˜• ë¶„ë¥˜ ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"camping_type_preference": "ìœ ë£Œìº í•‘ì¥", "error_message": str(e)}

# --- âœ… í†µí•©ëœ RAG + ì›¹ê²€ìƒ‰ ë…¸ë“œ ---
def search_camping(state: dict, camping_type: str, vectordb) -> dict:
    """ìµœì  í•˜ì´ë¸Œë¦¬ë“œ: ChromaDB ì €ì¥ëœ dense + BGE-M3 sparseë§Œ ìƒˆë¡œ ê³„ì‚°"""
    
    search_query = f"{state.get('original_question', '')} {state.get('question', '')}".strip()
    print(f"\n--- ğŸ” ìµœì  í•˜ì´ë¸Œë¦¬ë“œ: ì €ì¥ëœ dense + ìƒˆ sparse (í•„í„°: {camping_type}) ---")
    
    try:
        # 1ë‹¨ê³„: ChromaDBì—ì„œ ì €ì¥ëœ dense ë²¡í„°ë“¤ê³¼ ë¬¸ì„œ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        collection = vectordb._collection
        
        # ì¿¼ë¦¬ë§Œ denseë¡œ ì„ë² ë”©
        query_dense = unified_embedder.encode_for_vector_db([search_query])[0]
        
        # ChromaDBì—ì„œ ê¸°ì¡´ dense ë²¡í„°ë“¤ê³¼ í•¨ê»˜ ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        chroma_results = collection.query(
            query_embeddings=[query_dense.tolist()],
            n_results=500,  # ì¶©ë¶„íˆ ê°€ì ¸ì™€ì„œ sparseë¡œ ì¬ìˆœìœ„
            where={"ìº í•‘ìœ í˜•": camping_type},
            include=['documents', 'metadatas', 'embeddings', 'distances']
        )
        
        if not chroma_results['documents'][0]:
            return {"locations": []}
        
        documents = chroma_results['documents'][0]
        metadatas = chroma_results['metadatas'][0]
        stored_dense_vectors = np.array(chroma_results['embeddings'][0])  # ğŸ¯ ì´ë¯¸ ì €ì¥ëœ dense!
        
        print(f"ğŸ“„ ChromaDBì—ì„œ {len(documents)}ê°œ ë¬¸ì„œì˜ ê¸°ì¡´ dense ë²¡í„° ë¡œë“œ")
        
        # 2ë‹¨ê³„: sparseë§Œ ìƒˆë¡œ ê³„ì‚° (í…ìŠ¤íŠ¸ í•„ìš”)
        print("ğŸ”„ ì¿¼ë¦¬ + ë¬¸ì„œë“¤ì˜ sparse embeddingë§Œ ê³„ì‚°...")
        all_texts = [search_query] + documents
        print(search_query)
        sparse_only_embeddings = unified_embedder.model.encode(
            all_texts,
            batch_size=32,
            return_dense=False,     # âŒ denseëŠ” ì´ë¯¸ ìˆìœ¼ë‹ˆ ê³„ì‚° ì•ˆí•¨!
            return_sparse=True,     # âœ… sparseë§Œ ê³„ì‚°
            return_colbert_vecs=False
        )
        
        query_sparse = sparse_only_embeddings['lexical_weights'][0]
        doc_sparse_list = sparse_only_embeddings['lexical_weights'][1:]
        
        # 3ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        print("ğŸ¯ Dense(ì €ì¥ë¨) + Sparse(ìƒˆê³„ì‚°) í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°...")
        hybrid_scores = []
        
        for i, doc_sparse in enumerate(doc_sparse_list):
            # Dense ì ìˆ˜: ì´ë¯¸ ì €ì¥ëœ ë²¡í„° ì‚¬ìš©
            doc_dense = stored_dense_vectors[i]
            dense_score = float(np.dot(query_dense, doc_dense) / 
                              (np.linalg.norm(query_dense) * np.linalg.norm(doc_dense)))
            
            # Sparse ì ìˆ˜: ìƒˆë¡œ ê³„ì‚°ëœ ê²ƒ ì‚¬ìš©
            sparse_score = 0.0
            for token_id, query_weight in query_sparse.items():
                if token_id in doc_sparse:
                    sparse_score += query_weight * doc_sparse[token_id]
            
            # BGE-M3 í•˜ì´ë¸Œë¦¬ë“œ ê³µì‹
            final_score = dense_score * 1.0 + sparse_score * 0.2
            
            # Document ê°ì²´ ì¬êµ¬ì„±
            doc_obj = Document(
                page_content=documents[i],
                metadata=metadatas[i]
            )
            
            hybrid_scores.append((doc_obj, final_score, dense_score, sparse_score))
            print(f"[DEBUG] ë¬¸ì„œ {i+1}: {doc_obj.metadata.get('ìº í•‘ì¥ì´ë¦„','ì´ë¦„ì—†ìŒ')}, "f"Hybrid: {final_score:.3f}, Dense: {dense_score:.3f}, Sparse: {sparse_score:.4f}")
        # 4ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ë¡œ ì •ë ¬í•´ì„œ ìƒìœ„ 2ê°œ
        hybrid_scores.sort(key=lambda x: x[1], reverse=True)
        top_2_results = hybrid_scores[:2]

        print(f"ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê¸°ì¤€ ìƒìœ„ 2ê°œ:")
        for i, (doc, final_score, dense_score, sparse_score) in enumerate(top_2_results):
            print(f"[Top {i+1}] {doc.metadata.get('ìº í•‘ì¥ì´ë¦„', 'ì´ë¦„ì—†ìŒ')} "
                f"Hybrid:{final_score:.3f} (Dense:{dense_score:.3f} Sparse:{sparse_score:.4f})")

        # 5ë‹¨ê³„: ì›¹ ìŠ¤ë‹ˆí« ì¶”ê°€
        web_snippets_map = {}
        if camping_type != "ì˜¤ì§€/ë…¸ì§€ìº í•‘" and top_2_results:
            docs_with_metadata = [(doc, score) for doc, score, _, _ in top_2_results]
            
            snippets = asyncio.run(build_snippet_per_doc(
                docs_with_metadata=docs_with_metadata,
                per_type_display=20,
                fetch_timeout=8,
                max_chars=2000,
                enforce_name_in_title=True,
                include_when_no_local_modified=True,
            ))
            
            # ğŸ”¹ ë„¤ì´ë²„ ìŠ¤ë‹ˆí« ë””ë²„ê·¸ ì¶œë ¥
            for s in snippets:
                print(f"[DEBUG][ë„¤ì´ë²„ ìŠ¤ë‹ˆí«] ì¥ì†Œ: {s['ì¥ì†Œì´ë¦„']}, "
                    f"ë‚ ì§œ: {s.get('ë‚ ì§œ')}, snippet ê¸¸ì´: {len(s.get('snippet',''))}")

            # ì¥ì†Œ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
            web_snippets_map = {s['ì¥ì†Œì´ë¦„']: s for s in snippets}

        # 6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ë°˜í™˜
        locations_data = []
        unique_names = set()

        for doc, final_score, _, _ in top_2_results:
            meta = getattr(doc, "metadata", {}) or {}
            location_name = meta.get("ìº í•‘ì¥ì´ë¦„", "ì´ë¦„ ì •ë³´ ì—†ìŒ")
            
            if location_name in unique_names:
                continue
            unique_names.add(location_name)

            location_info = {
                "local_document": {
                    "metadata": meta, 
                    "content": doc.page_content, 
                    "score": final_score
                },
                "web_snippet": web_snippets_map.get(location_name)  # ğŸ”¹ ì—¬ê¸°ì— ë„¤ì´ë²„ ìŠ¤ë‹ˆí« ì¶”ê°€
            }
            locations_data.append(location_info)

        print(f"[DEBUG] bge_new.py: search_camping, returning {len(locations_data)} locations")
        return {"locations": locations_data}
        
    except Exception as e:
        print(f"âŒ ìµœì  í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return {"locations": []}


def search_paid_camping(state):
    return search_camping(state, "ìœ ë£Œìº í•‘ì¥", vectordb)

def search_glamping_caravan(state):
    return search_camping(state, "ê¸€ë¨í•‘/ì¹´ë¼ë°˜", vectordb)

def search_ojee_camping(state):
    return search_camping(state, "ì˜¤ì§€/ë…¸ì§€ìº í•‘", vectordb)

# --- ì¥ì†Œ ì¶”ì²œ ìµœì¢… ë‹µë³€ ìƒì„± ---
def generate_location_answer(state: GraphState) -> dict:
    t0 = time.perf_counter()
    
    original_question = state.get("original_question", "")
    second_question = state.get("question", "")
    camping_type = state.get("camping_type_preference", "")
    context = state.get("context", [])
    locations = state.get("locations", [])
    print(f"[DEBUG] ìµœì¢… ë°˜í™˜ë  ì¥ì†Œ ê°œìˆ˜: {len(locations)}ê°œ")
    
    context_strs = []
    final_locations = []
    for loc in locations:
        local_meta = loc.get("local_document", {}).get("metadata", {})
        local_content = loc.get("local_document", {}).get("content", "")
        snippet = loc.get("web_snippet", {})
        snippet_text = snippet.get("snippet", "") if snippet else ""
        
        # ì˜¬ë°”ë¥¸ ë©”íƒ€ë°ì´í„° ì ‘ê·¼ ë° final_locations ìƒì„±
        location_data = {
            "name": local_meta.get('ìº í•‘ì¥ì´ë¦„'),
            "address": local_meta.get('ìº í•‘ì¥ì£¼ì†Œ'),
            "latitude": local_meta.get('ìº í•‘ì¥ ìœ„ë„'),
            "longitude": local_meta.get('ìº í•‘ì¥ ê²½ë„'),
            "local_meta": local_meta,
        }
        final_locations.append(location_data)
        
        context_strs.append(
            f"ë©”íƒ€ë°ì´í„°: {final_locations}"
            f"ë³¸ë¬¸: {local_content}"
            f"ë„¤ì´ë²„ ì •ë³´: {snippet_text}"
        )

    context_str = "\n---\n".join(context_strs[:2])

    prompt = (
        f"ë‹¹ì‹ ì€ ìº í•‘ì—ì´ì „íŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ '{camping_type}' ìœ í˜•ì— ë§ëŠ” ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”. ìµœëŒ€í•œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ì¶”ì²œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ ë‚´ ë©”íƒ€ë°ì´í„°ì™€ ìµœì‹  ë„¤ì´ë²„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì‚¬ì´íŠ¸ë‚˜ ì „í™”ë²ˆí˜¸ë¥¼ ë§í•´ì¤„ë•ŒëŠ” \"ëª¨ë“  ì •ë³´ëŠ” ìµœì‹ ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸/ì „í™”ë¡œ ì¬í™•ì¸ ë°”ëë‹ˆë‹¤.\"ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.\n\n"
        
        f"ì²« ë²ˆì§¸ ì§ˆë¬¸: {original_question}\n"
        f"ë‘ ë²ˆì§¸ ì§ˆë¬¸ ë° ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ë‹µë³€: {second_question}\n\n"
        
        f"ë¬¸ë§¥:\n{context_str}\n\n"
        
        "ìœ„ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ì¤„ë°”ê¿ˆì„ ì´ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        "ë‹µë³€:"
    )
    try:
        out = oai_text(prompt)
        answer = out["text"]
        print(f"âœ… ì¥ì†Œ ì¶”ì²œ ë‹µë³€ ìƒì„± ì™„ë£Œ ({time.perf_counter() - t0:.2f}s | req={out['request_id']})")
        return {"final_answer": answer, "locations": final_locations}
    except Exception as e:
        print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e} ({time.perf_counter() - t0:.2f}s)")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e), "locations": final_locations}

# --- ë©”ì¸ ì›Œí¬í”Œë¡œìš° ---
workflow = StateGraph(GraphState)
workflow.add_node("classify_type", classify_question_type)
workflow.add_node("generate_general", generate_general_answer)
workflow.add_node("ask_preference", ask_camping_preference)
workflow.set_entry_point("classify_type")
workflow.add_conditional_edges(
    "classify_type",
    route_by_classification,
    {
        "general": "generate_general",
        "ask_preference": "ask_preference",
    },
)
workflow.add_edge("generate_general", END)
workflow.add_edge("ask_preference", END)

# --- ìº í•‘ ìœ í˜• í›„ì† ì›Œí¬í”Œë¡œìš° ---
continuation = StateGraph(GraphState)
continuation.add_node("classify_camping", classify_camping_type)
continuation.add_node("search_paid", search_paid_camping)
continuation.add_node("search_glamping", search_glamping_caravan)
continuation.add_node("search_ojee", search_ojee_camping)
continuation.add_node("generate_location", generate_location_answer)
continuation.set_entry_point("classify_camping")
continuation.add_conditional_edges(
    "classify_camping",
    route_by_camping_type,
    {
        "paid": "search_paid",
        "glamping": "search_glamping",
        "ojee": "search_ojee",
    },
)
continuation.add_edge("search_paid", "generate_location")
continuation.add_edge("search_glamping", "generate_location")
continuation.add_edge("search_ojee", "generate_location")
continuation.add_edge("generate_location", END)

main_app = workflow.compile()
continuation_app = continuation.compile()

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---

def main():
    print("ğŸ•ï¸ ìº í•‘ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    waiting_for_camping_choice = False
    original_q_cache: str | None = None

    while True:
        user_input = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if user_input.lower() in ["ì¢…ë£Œ", "quit", "exit"]:
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        try:
            if waiting_for_camping_choice:
                # ìº í•‘ ìœ í˜• ì…ë ¥ ëŒ€ê¸° ì¤‘
                state: GraphState = {
                    "question": user_input,
                    "original_question": original_q_cache or user_input,
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                }
                result = asyncio.run(continuation_app.ainvoke(state))
                waiting_for_camping_choice = False
                original_q_cache = None
            else:
                # ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬
                state: GraphState = {
                    "question": user_input,
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                }
                result = asyncio.run(main_app.ainvoke(state))

                # ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ìœ í˜•ì„ ë°›ë„ë¡ ì „í™˜
                if "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?" in result.get("final_answer", ""):
                    waiting_for_camping_choice = True
                    original_q_cache = state["question"]

            print(f"\nğŸ“ ë‹µë³€: {result.get('final_answer')}\n")

        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            waiting_for_camping_choice = False
            original_q_cache = None

if __name__ == "__main__":
    main()
