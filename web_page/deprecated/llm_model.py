from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer
import torch


class LLMModel:
    def __init__(self, model_name="K-intelligence/Midm-2.0-Mini-Instruct", multi_turn=False, use_cuda=True):
        self.device = "cuda" if torch.cuda.is_available() and use_cuda else "cpu"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 if self.device == "cuda" else torch.float32).to(self.device)
        self.multi_turn = multi_turn
        self.history = []  # [(user, bot), ...]

    def build_prompt(self, user_input):
        if not self.multi_turn or not self.history:
            return f"### User: {user_input}\n### Assistant:"
        else:
            history_text = ""
            for user, bot in self.history:
                history_text += f"### User: {user}\n### Assistant: {bot}\n"
            return history_text + f"### User: {user_input}\n### Assistant:"

    def chat(self, user_input, max_new_tokens=512):
        prompt = self.build_prompt(user_input)
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        inputs.pop("token_type_ids", None)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            pad_token_id=self.tokenizer.eos_token_id
        )
        full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = full_output.split("### Assistant:")[-1].strip()

        if self.multi_turn:
            self.history.append((user_input, answer))

        return answer

    def reset_history(self):
        self.history = []


if __name__ == "__main__":
    model = LLMModel(multi_turn=True)  # ë˜ëŠ” False
    print("LLM ì±—ë´‡ ì‹œì‘! ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥")

    while True:
        user_input = input("You: ")
        if user_input.lower() in ("exit", "quit"):
            break
        response = model.chat(user_input)
        print("Bot:", response)






# # llm_model.py
# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch

# class LLMChatModel:
#     def __init__(self, model_name="K-intelligence/Midm-2.0-Mini-Instruct", device = None):
#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)
#         self.model = AutoModelForCausalLM.from_pretrained(
#             model_name,
#             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
#             device_map="auto" if torch.cuda.is_available() else None
#         )
#         self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#         # ì±„íŒ… íˆìŠ¤í† ë¦¬
#         self.chat_history = []

#         # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì • (ì„ íƒì )
#         self.system_prompt = "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."

#     def generate_prompt(self, user_input):
#         prompt = ""
#         if self.system_prompt:
#             prompt += f"[System]\n{self.system_prompt}\n"

#         for user, bot in self.chat_history:
#             prompt += f"[User]: {user}\n[Assistant]: {bot}\n"

#         prompt += f"[User]: {user_input}\n[Assistant]:"
#         return prompt

#     def chat(self, user_input, max_new_tokens=512):
#         prompt = self.generate_prompt(user_input)

#         inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
#         # token_type_ids ì œê±°
#         if "token_type_ids" in inputs:
#             inputs.pop("token_type_ids")

#         outputs = self.model.generate(
#             **inputs,
#             max_new_tokens=max_new_tokens,
#             do_sample=True,
#             temperature=0.7,
#             top_p=0.9,
#             repetition_penalty=1.1,
#             pad_token_id=self.tokenizer.eos_token_id
#         )

#         output_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

#         # ë§ˆì§€ë§‰ Assistant ì‘ë‹µë§Œ ì¶”ì¶œ
#         response = output_text.split("[Assistant]:")[-1].strip()

#         # íˆìŠ¤í† ë¦¬ ê°±ì‹ 
#         self.chat_history.append((user_input, response))
#         return response


# # CLI í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
# if __name__ == "__main__":
#     chat_model = LLMChatModel()

#     print("ğŸ”¹ K-intelligence Midm-2.0-Mini-Instruct Chatbot ğŸ”¹")
#     while True:
#         try:
#             user_input = input("You: ")
#             if user_input.lower() in ["exit", "quit"]:
#                 break
#             response = chat_model.chat(user_input)
#             print("AI:", response)
#         except KeyboardInterrupt:
#             print("\nì¢…ë£Œí•©ë‹ˆë‹¤.")
#             break
