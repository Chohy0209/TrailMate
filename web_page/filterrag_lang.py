import operator
from typing import TypedDict, Annotated, List, Any
import asyncio
import requests
import time
import random
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import HuggingFacePipeline
from langgraph.graph import StateGraph, END
from langchain.schema import Document

# ì„¤ì •
MAX_LOOPS = 2
LLM_TIMEOUT = 30
MODEL_DIR = "./model_ax_merge4"
CLASSIFIER_MODEL_NAME = "./model_ax_merge4"
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large-instruct"
CHROMA_DB_PATH = './data/camp_chroma_store'

class GraphState(TypedDict):
    question: str
    original_question: str
    classification: str
    camping_type_preference: str
    context: Annotated[List[Any], operator.add]
    locations: Annotated[List[dict], operator.add] # locations ì¶”ê°€
    final_answer: str
    loop_count: int
    search_attempted: bool
    error_message: str

# ëª¨ë¸ ë¡œë”© (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
print("--- LLM ëª¨ë¸ ë¡œë”© ì¤‘ ---")
start_time = time.perf_counter()

model_main = AutoModelForCausalLM.from_pretrained(MODEL_DIR, device_map="auto", torch_dtype=torch.float16)
tokenizer_main = AutoTokenizer.from_pretrained(MODEL_DIR)
if tokenizer_main.pad_token is None:
    tokenizer_main.pad_token = tokenizer_main.eos_token
tokenizer_main.pad_token = tokenizer_main.eos_token

llm_pipeline_main = pipeline("text-generation", model=model_main, tokenizer=tokenizer_main, max_new_tokens=1024, do_sample=True, temperature=0.7, pad_token_id=tokenizer_main.eos_token_id)
llm_main = HuggingFacePipeline(pipeline=llm_pipeline_main)

model_classifier = AutoModelForCausalLM.from_pretrained(CLASSIFIER_MODEL_NAME, device_map="auto", torch_dtype=torch.float16)
tokenizer_classifier = AutoTokenizer.from_pretrained(CLASSIFIER_MODEL_NAME)
if tokenizer_classifier.pad_token is None:
    tokenizer_classifier.pad_token = tokenizer_classifier.eos_token

llm_pipeline_classifier = pipeline("text-generation", model=model_classifier, tokenizer=tokenizer_classifier, max_new_tokens=100, do_sample=False, temperature=0.0, pad_token_id=tokenizer_classifier.eos_token_id)
llm_classifier = HuggingFacePipeline(pipeline=llm_pipeline_classifier)
end_time = time.perf_counter()
print(f"--- ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")

print("--- ì„ë² ë”© ëª¨ë¸ ë° ChromaDB ë¡œë”© ì¤‘ ---")
start_time = time.perf_counter()
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
vectordb = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
retriever = vectordb.as_retriever(search_kwargs={"k": 5})
end_time = time.perf_counter()
print(f"--- ì„ë² ë”© ëª¨ë¸ ë° ChromaDB ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")

def extract_llm_response(raw_response: str, prompt: str) -> str:
    response_part = raw_response.split(prompt)[-1].strip()
    first_line = response_part.split('\n')[0].strip()
    return first_line

def extract_locations_from_docs(docs: List[Document]) -> List[dict]:
    locations = []
    for doc in docs:
        if hasattr(doc, 'metadata') and 'ìœ„ë„' in doc.metadata and 'ê²½ë„' in doc.metadata:
            name = doc.page_content.split('\n')[0].strip()
            locations.append({
                "name": name,
                "lat": doc.metadata['ìœ„ë„'],
                "lon": doc.metadata['ê²½ë„']
            })
    return locations

# ===== ë…¸ë“œ 1: 1ì°¨ ë¶„ë¥˜ (ì¼ë°˜ vs ì¥ì†Œì¶”ì²œ) =====
def classify_question_type(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ1] ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (ì¼ë°˜ vs ì¥ì†Œì¶”ì²œ) ---")
    question = state['question']
    prompt = f"""ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  'ì¼ë°˜ ìº í•‘' ë˜ëŠ” 'ì¥ì†Œ ì¶”ì²œ' ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

<ë¶„ë¥˜ ê¸°ì¤€>
- ì¼ë°˜ ìº í•‘: ìº í•‘ ì¤€ë¹„ë¬¼, ì¥ë¹„, íŒ, ë°©ë²• ë“± ìº í•‘ ìì²´ì— ëŒ€í•œ ì§ˆë¬¸
- ì¥ì†Œ ì¶”ì²œ: ì¥ì†Œë¥¼ ì¶”ì²œí•´ë‹¬ë¼ëŠ” ì§ˆë¬¸ì´ë‚˜ ì¥ì†Œë¥¼ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸

ì§ˆë¬¸: {question}
ë¶„ë¥˜:"""
    
    try:
        raw_response = llm_classifier.invoke(prompt)
        classification = extract_llm_response(raw_response, prompt)
        if classification not in ['ì¼ë°˜ ìº í•‘', 'ì¥ì†Œ ì¶”ì²œ']:
            classification = 'ì¼ë°˜ ìº í•‘'
            print("âš ï¸ ë¶„ë¥˜ ê²°ê³¼ê°€ ë¶ˆë¶„ëª…í•˜ì—¬ 'ì¼ë°˜ ìº í•‘'ìœ¼ë¡œ ê¸°ë³¸ ì²˜ë¦¬")
        print(f"ë¶„ë¥˜ ê²°ê³¼: {classification}")
        return {
            "classification": classification,
            "original_question": question
        }
    except Exception as e:
        print(f"ë¶„ë¥˜ ì˜¤ë¥˜: {e}. 'ì¼ë°˜ ìº í•‘'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return {
            "classification": "ì¼ë°˜ ìº í•‘",
            "error_message": str(e),
            "original_question": question
        }

# ===== ë…¸ë“œ 2: ì¼ë°˜ ì§ˆë¬¸ ë‹µë³€ ìƒì„± =====
def generate_general_answer(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ2] ì¼ë°˜ ìº í•‘ ì§ˆë¬¸ ë‹µë³€ ìƒì„± ---")
    question = state['question']
    prompt = f"""ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìº í•‘ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}
ë‹µë³€:"""
    
    try:
        response = llm_pipeline_main(prompt, max_new_tokens=512)[0]['generated_text']
        final_answer = response.replace(prompt, "").strip()
        return {"final_answer": final_answer}
    except Exception as e:
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e)}

# ===== ë…¸ë“œ 3: ìº í•‘ ìœ í˜• ì„ íƒ ìš”ì²­ (3ê°€ì§€ë¡œ ì„¸ë¶„í™”) =====
def ask_camping_preference(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ3] ìº í•‘ ìœ í˜• ì„ íƒ ìš”ì²­ ---")
    clarification_message = """ğŸ•ï¸ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”! 
ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?

ex)

1ï¸âƒ£ **ìœ ë£Œìº í•‘ì¥** - ì˜¤í† ìº í•‘ì¥, ì¼ë°˜ ìº í•‘ì¥, í¸ì˜ì‹œì„¤ ì™„ë¹„
2ï¸âƒ£ **ê¸€ë¨í•‘/ì¹´ë¼ë°˜** - ê¸€ë¨í•‘ì¥, ì¹´ë¼ë°˜, íœì…˜í˜• ìº í•‘ 
3ï¸âƒ£ **ì˜¤ì§€/ë…¸ì§€ìº í•‘** - ìì—° ì† ì•¼ìƒìº í•‘, ë°±íŒ¨í‚¹, ë¬´ë£Œ ìº í•‘
"""
    return {"final_answer": clarification_message}

# ===== ë…¸ë“œ 4: ìº í•‘ ìœ í˜• ë¶„ë¥˜ (3ê°€ì§€ë¡œ ì„¸ë¶„í™”) =====
def classify_camping_type(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ4] ìº í•‘ ìœ í˜• ë¶„ë¥˜ (ìœ ë£Œ/ê¸€ë¨í•‘/ì˜¤ì§€) ---")
    user_input = state['question']
    prompt = f"""ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ì„ í˜¸ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì‘ë‹µì„ ì½ê³  'ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘ì¹´ë¼ë°˜', 'ì˜¤ì§€ë…¸ì§€ìº í•‘' ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

<ë¶„ë¥˜ ê¸°ì¤€>
- ìœ ë£Œìº í•‘ì¥: 1, 1ë²ˆ, ìœ ë£Œ, ì˜¤í† ìº í•‘ì¥, ì¼ë°˜ìº í•‘ì¥, í¸ì˜ì‹œì„¤ì„ ì›í•˜ëŠ” ê²½ìš°
- ê¸€ë¨í•‘ì¹´ë¼ë°˜: 2, 2ë²ˆ, ê¸€ë¨í•‘, ì¹´ë¼ë°˜, íœì…˜, ëŸ­ì…”ë¦¬, í¸ì•ˆí•œ ìº í•‘,ìº í•‘ì¹´ë¥¼ë¥¼ ì›í•˜ëŠ” ê²½ìš°  
- ì˜¤ì§€ë…¸ì§€ìº í•‘: 3, 3ë²ˆ, ì˜¤ì§€, ë…¸ì§€, ì•¼ìƒìº í•‘, ë°±íŒ¨í‚¹, ìì—° ì† ìº í•‘, ë¬´ë£Œë¥¼ ì›í•˜ëŠ” ê²½ìš°

ì‚¬ìš©ì ì‘ë‹µ: {user_input}
ë¶„ë¥˜:"""
    
    try:
        raw_response = llm_classifier.invoke(prompt)
        camping_type = extract_llm_response(raw_response, prompt)
        if camping_type not in ['ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘ì¹´ë¼ë°˜', 'ì˜¤ì§€ë…¸ì§€ìº í•‘']:
            camping_type = 'ìœ ë£Œìº í•‘ì¥'
            print("âš ï¸ ë¶„ë¥˜ ê²°ê³¼ê°€ ë¶ˆë¶„ëª…í•˜ì—¬ 'ìœ ë£Œìº í•‘ì¥'ìœ¼ë¡œ ê¸°ë³¸ ì²˜ë¦¬")
        print(f"ìº í•‘ ìœ í˜• ë¶„ë¥˜ ê²°ê³¼: {camping_type}")
        return {"camping_type_preference": camping_type}
    except Exception as e:
        print(f"ìº í•‘ ìœ í˜• ë¶„ë¥˜ ì˜¤ë¥˜: {e}. 'ìœ ë£Œìº í•‘ì¥'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return {
            "camping_type_preference": "ìœ ë£Œìº í•‘ì¥",
            "error_message": str(e)
        }

# ===== ë…¸ë“œ 5: ìœ ë£Œìº í•‘ì¥ ê²€ìƒ‰ =====
async def search_paid_camping(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ5] ìœ ë£Œìº í•‘ì¥ RAG+ì›¹ê²€ìƒ‰ ---")
    original_question = state.get('original_question', state['question'])
    try:
        docs = await asyncio.to_thread(
            vectordb.similarity_search,
            query=original_question,
            k=2,
            filter={"ìº í•‘ìœ í˜•": 'ìœ ë£Œìº í•‘ì¥'}
        )
        context = docs.copy()
        locations = extract_locations_from_docs(docs)
        print(f"ìœ ë£Œìº í•‘ì¥ ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ, ìœ„ì¹˜ {len(locations)}ê°œ")

        web_results = await general_web_search_async(original_question, "ìœ ë£Œìº í•‘ì¥")
        context.extend(web_results)
        print(f"ì›¹ê²€ìƒ‰ ê²°ê³¼: {len(web_results)}ê°œ ì¶”ê°€")

        return {"context": context, "locations": locations, "search_attempted": True}
    except Exception as e:
        print(f"ìœ ë£Œìº í•‘ì¥ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            "context": ["ìœ ë£Œìº í•‘ì¥ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"],
            "locations": [],
            "error_message": str(e),
            "search_attempted": True
        }

# ===== ë…¸ë“œ 6: ê¸€ë¨í•‘/ì¹´ë¼ë°˜ ì›¹ê²€ìƒ‰+RAG =====
async def search_glamping_caravan(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ6] ê¸€ë¨í•‘/ì¹´ë¼ë°˜ RAG+ì›¹ê²€ìƒ‰ ---")
    original_question = state.get('original_question', state['question'])
    try:
        docs = await asyncio.to_thread(
            vectordb.similarity_search,
            query=original_question,
            k=5,
            filter={"ìº í•‘ìœ í˜•": 'ê¸€ë¨í•‘/ì¹´ë¼ë°˜'}
        )
        context = docs.copy()
        locations = extract_locations_from_docs(docs)
        print(f"ê¸€ë¨í•‘/ì¹´ë¼ë°˜ RAG ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ, ìœ„ì¹˜ {len(locations)}ê°œ")

        web_results = await general_web_search_async(original_question, "ê¸€ë¨í•‘/ì¹´ë¼ë°˜")
        context.extend(web_results)
        print(f"ì›¹ê²€ìƒ‰ ê²°ê³¼: {len(web_results)}ê°œ ì¶”ê°€")

        return {"context": context, "locations": locations, "search_attempted": True}
    except Exception as e:
        print(f"ê¸€ë¨í•‘/ì¹´ë¼ë°˜ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            "context": ["ê¸€ë¨í•‘/ì¹´ë¼ë°˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"],
            "locations": [],
            "error_message": str(e),
            "search_attempted": True
        }

# ===== ë…¸ë“œ 7: ì˜¤ì§€/ë…¸ì§€ìº í•‘ ë²¡í„°DB ê²€ìƒ‰ =====
async def search_ojee_camping(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ7] ì˜¤ì§€/ë…¸ì§€ìº í•‘ RAG ê²€ìƒ‰ ---")
    original_question = state.get('original_question', state['question'])
    try:
        docs = await asyncio.to_thread(
            vectordb.similarity_search,
            query=original_question,
            k=5,
            filter={"ìº í•‘ìœ í˜•": 'ì˜¤ì§€/ë…¸ì§€ìº í•‘'}
        )
        locations = extract_locations_from_docs(docs)
        print(f"ì˜¤ì§€/ë…¸ì§€ìº í•‘ ê²€ìƒ‰ ê²°ê³¼: {len(docs)}ê°œ ë¬¸ì„œ, ìœ„ì¹˜ {len(locations)}ê°œ")
        return {"context": docs.copy(), "locations": locations, "search_attempted": True}
    except Exception as e:
        print(f"ì˜¤ì§€/ë…¸ì§€ìº í•‘ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {
            "context": ["ì˜¤ì§€/ë…¸ì§€ìº í•‘ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"],
            "locations": [],
            "error_message": str(e),
            "search_attempted": True
        }

# ì›¹ê²€ìƒ‰ í•¨ìˆ˜ë“¤
async def general_web_search_async(query: str, camping_type: str) -> List[str]:
    await asyncio.sleep(0.5)  # ëª¨ì˜ API ëŒ€ê¸°ì‹œê°„

    mock_results = {
        "ìœ ë£Œìº í•‘ì¥": [
            "[ì›¹ê²€ìƒ‰] ì†ì´ˆ ì˜¤í† ìº í•‘ì¥ - ë°”ë‹·ê°€ ê·¼ì²˜, ì „ê¸°ì‹œì„¤ ìˆìŒ",
            "[ì›¹ê²€ìƒ‰] ê°•ë¦‰ í•´ë³€ ìº í•‘ì¥ - ìƒ¤ì›Œì¥, ë§¤ì  ì™„ë¹„",
            "[ì›¹ê²€ìƒ‰] ì–‘ì–‘ ì¼ë°˜ ìº í•‘ì¥ - ì˜ˆì•½ ê°€ëŠ¥, í™”ì¥ì‹¤ ìˆìŒ"
        ],
        "ê¸€ë¨í•‘/ì¹´ë¼ë°˜": [
            "[ì›¹ê²€ìƒ‰] ê°€í‰ ì¹´ë¼ë°˜ ìº í•‘ì¥ - ë°”ë² í, ì¹¨ëŒ€ ì™„ë¹„",
            "[ì›¹ê²€ìƒ‰] ì œì£¼ ê¸€ë¨í•‘ ë¹Œë¦¬ì§€ - ì˜¤ì…˜ë·°, ì˜¨ìˆ˜ìƒ¤ì›Œ",
            "[ì›¹ê²€ìƒ‰] ë‚¨í•´ ê¸€ë¨í•‘ - í•´ì•ˆì ˆê²½, ëŸ­ì…”ë¦¬ í…íŠ¸"
        ]
    }

    return random.sample(mock_results.get(camping_type, []), k=3)

# ===== ë…¸ë“œ 8: ì¥ì†Œì¶”ì²œ ë‹µë³€ ìƒì„± =====
def generate_location_answer(state: GraphState) -> dict:
    print("--- [ë…¸ë“œ8] ì¥ì†Œì¶”ì²œ ë‹µë³€ ìƒì„± ---")
    original_question = state.get('original_question', state['question'])
    camping_type = state.get('camping_type_preference', '')
    context = state.get('context', [])
    context_str = "\n".join(
        ctx.page_content if hasattr(ctx, 'page_content') else str(ctx) for ctx in context[:5]
    )
    
    prompt = f"""ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ {camping_type} ì¶”ì²œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”.

ìº í•‘ ìœ í˜•: {camping_type}
ë¬¸ë§¥:
{context_str}

ì§ˆë¬¸: {original_question}
ë‹µë³€:"""
    
    try:
        response = llm_pipeline_main(prompt, max_new_tokens=512)[0]['generated_text']
        final_answer = response.replace(prompt, "").strip()
        # locationsëŠ” ì´ë¯¸ stateì— ìˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬ë¨
        return {"final_answer": final_answer}
    except Exception as e:
        return {"final_answer": "ì¥ì†Œ ì¶”ì²œ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e)}

# ===== ë¼ìš°íŒ… í•¨ìˆ˜ë“¤ =====
def route_by_classification(state: GraphState) -> str:
    """1ì°¨ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
    if state['classification'] == 'ì¼ë°˜ ìº í•‘':
        return 'general'
    else:
        return 'location'

def route_by_camping_type(state: GraphState) -> str:
    """ìº í•‘ ìœ í˜•ì— ë”°ë¥¸ ë¼ìš°íŒ… (3ê°€ì§€)"""
    camping_type = state['camping_type_preference']
    if camping_type == 'ìœ ë£Œìº í•‘ì¥':
        return 'paid'
    elif camping_type == 'ê¸€ë¨í•‘ì¹´ë¼ë°˜':
        return 'glamping'
    else:  # ì˜¤ì§€ë…¸ì§€ìº í•‘
        return 'ojee'

# ===== LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± =====
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("classify_type", classify_question_type)           # ë…¸ë“œ1: 1ì°¨ ë¶„ë¥˜
workflow.add_node("generate_general", generate_general_answer)       # ë…¸ë“œ2: ì¼ë°˜ ë‹µë³€
workflow.add_node("ask_preference", ask_camping_preference)          # ë…¸ë“œ3: ì„ íƒ ìš”ì²­ (3ê°€ì§€)

# ì‹œì‘ì  ì„¤ì •
workflow.set_entry_point("classify_type")

# ì—£ì§€ ì—°ê²°
workflow.add_conditional_edges("classify_type", route_by_classification, {
    "general": "generate_general",
    "location": "ask_preference"
})

workflow.add_edge("generate_general", END)
workflow.add_edge("ask_preference", END)

# ë³„ë„ì˜ continuation ì›Œí¬í”Œë¡œìš° (ì‚¬ìš©ì ì‘ë‹µ ì²˜ë¦¬ìš©)
continuation_workflow = StateGraph(GraphState)
continuation_workflow.add_node("classify_camping", classify_camping_type)
continuation_workflow.add_node("search_paid", search_paid_camping)
continuation_workflow.add_node("search_glamping", search_glamping_caravan)
continuation_workflow.add_node("search_ojee", search_ojee_camping)
continuation_workflow.add_node("generate_location", generate_location_answer)

continuation_workflow.set_entry_point("classify_camping")
continuation_workflow.add_conditional_edges("classify_camping", route_by_camping_type, {
    "paid": "search_paid",
    "glamping": "search_glamping", 
    "ojee": "search_ojee"
})
continuation_workflow.add_edge("search_paid", "generate_location")
continuation_workflow.add_edge("search_glamping", "generate_location")
continuation_workflow.add_edge("search_ojee", "generate_location")
continuation_workflow.add_edge("generate_location", END)

# ì•± ì»´íŒŒì¼
main_app = workflow.compile()
continuation_app = continuation_workflow.compile()

# ===== ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ =====
def main():
    print("\n" + "=" * 60)
    print("ğŸ•ï¸ ìº í•‘ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! 'ì¢…ë£Œ' ì…ë ¥ ì‹œ ì¢…ë£Œë©ë‹ˆë‹¤.")
    print("=" * 60)
    
    waiting_for_camping_choice = False
    last_state = None
    
    while True:
        user_input = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if user_input.lower() in ["ì¢…ë£Œ", "quit", "exit"]:
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        
        if waiting_for_camping_choice:
            # ìº í•‘ ìœ í˜• ì„ íƒ ì²˜ë¦¬
            state = {
                "question": user_input,
                "original_question": last_state.get('original_question', user_input),
                "context": [],
                "locations": [], # locations ì´ˆê¸°í™”
                "search_attempted": False
            }
            result = asyncio.run(continuation_app.ainvoke(state))
            waiting_for_camping_choice = False
            last_state = None
        else:
            # ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬
            state = {
                "question": user_input,
                "context": [],
                "locations": [], # locations ì´ˆê¸°í™”
                "search_attempted": False,
                "loop_count": 0
            }
            result = asyncio.run(main_app.ainvoke(state))
            
            # ìº í•‘ ìœ í˜• ì„ íƒ ìš”ì²­ì´ë©´ ëŒ€ê¸° ìƒíƒœë¡œ ì „í™˜
            if "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?" in result.get('final_answer', ''):
                waiting_for_camping_choice = True
                last_state = result
        
        print(f"\nğŸ“ ë‹µë³€: {result.get('final_answer')}\n")
        if result.get('locations'):
            print(f"ğŸ“ ì¶”ì²œ ì¥ì†Œ: {result['locations']}")

if __name__ == "__main__":
    main()
