# rag_components.py
import asyncio
import json
import operator
import time
from typing import Annotated, Any, Dict, List, TypedDict

from FlagEmbedding import BGEM3FlagModel
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph

import config
from naver_api import build_snippet_per_doc
from services import client, driver

# --- 1. BGE-M3 Embedder Class ---

class UnifiedBGEM3Embedder:
    """BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.model = None
        self.model_name = None
        self.lock = asyncio.Lock()  # ë™ì‹œ ì ‘ê·¼ ì œì–´ë¥¼ ìœ„í•œ Lock

    def _load_model_sync(self, model_name: str):
        """ë™ê¸°ì ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        self.model_name = model_name
        try:
            self.model = BGEM3FlagModel(self.model_name, use_fp16=True)
            print(f"âœ… BGE-M3 í†µí•© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")
        except Exception as e:
            print(f"âŒ BGE-M3 '{self.model_name}' ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            try:
                self.model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
                print("âœ… BGE-M3 ì›ë³¸ ëª¨ë¸ë¡œ Fallback ë¡œë“œ ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ BGE-M3 Fallbackë„ ì‹¤íŒ¨: {e2}")
                self.model = None

    @classmethod
    async def create(cls, model_name: str = config.EMBEDDING_MODEL_NAME):
        """ë¹„ë™ê¸°ì ìœ¼ë¡œ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ëª¨ë¸ì„ ë¡œë“œ"""
        embedder = cls()
        await asyncio.to_thread(embedder._load_model_sync, model_name)
        return embedder

    async def encode_for_vector_db(self, texts: List[str], task_id: str) -> List[List[float]]:
        """Neo4jìš© dense embeddingì„ ìƒì„± (ë¹„ë™ê¸° Lock í¬í•¨)"""
        if self.model is None:
            raise Exception("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        def _encode_sync():
            embeddings = self.model.encode(
                texts, batch_size=32, return_dense=True,
                return_sparse=False, return_colbert_vecs=False
            )
            return [arr.tolist() for arr in embeddings['dense_vecs']]

        print(f"  [TASK: {task_id}] ğŸŸ¡ ì„ë² ë”© Lock ëŒ€ê¸° ì‹œì‘...")
        async with self.lock:
            print(f"  [TASK: {task_id}] ğŸŸ¢ ì„ë² ë”© Lock í™•ë³´! ì¸ì½”ë”© ì‘ì—… ì‹œì‘...")
            start_time = time.time()
            result = await asyncio.to_thread(_encode_sync)
            duration = time.time() - start_time
            print(f"  [TASK: {task_id}] âœ… ì„ë² ë”© ì™„ë£Œ! ({duration:.2f}ì´ˆ)")
            return result


# --- 2. LangGraph State Definition ---

class GraphState(TypedDict):
    """LangGraphì˜ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” í´ë˜ìŠ¤"""
    question: str
    original_question: str
    classification: str
    camping_type_preference: str
    context: Annotated[List[Any], operator.add]
    locations: Annotated[List[dict], operator.add]
    final_answer: str
    error_message: str
    unified_embedder: UnifiedBGEM3Embedder # Embedderë¥¼ ìƒíƒœì— í¬í•¨


# --- 3. LangGraph Node Functions ---

async def oai_text(prompt: str) -> str:
    """OpenAI APIë¥¼ í˜¸ì¶œí•˜ì—¬ í…ìŠ¤íŠ¸ ì‘ë‹µì„ ë°›ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""
    resp = await client.chat.completions.create(
        model=config.GPT_MODEL, messages=[{"role": "user", "content": prompt}]
    )
    return (resp.choices[0].message.content or "").strip()

async def classify_question_type(state: GraphState) -> dict:
    """ì§ˆë¬¸ì˜ ìœ í˜•ì„ 'ì¼ë°˜ ìº í•‘'ê³¼ 'ì¥ì†Œ ì¶”ì²œ'ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ë…¸ë“œ"""
    prompt = (
        "ë‹¹ì‹ ì€ ìº í•‘ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë†€ëŸ¬ê°ˆ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë‹¬ë¼ê³  í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  ì¼ë°˜ ìº í•‘ì— ê´€í•œ ì§ˆë¬¸ì´ë©´ 'ì¼ë°˜ ìº í•‘' ìœ¼ë¡œ ë‹µí•˜ê³ , ë†€ëŸ¬ê°€ëŠ” ì¥ì†Œì— ê´€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ë§¥ìƒ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì€ 'ì¥ì†Œ ì¶”ì²œ'ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n"
        "'ì¼ë°˜ ìº í•‘', 'ì¥ì†Œ ì¶”ì²œ' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {state['question']}\në¶„ë¥˜:"
    )
    try:
        text = await oai_text(prompt)
        classification = "ì¥ì†Œ ì¶”ì²œ" if "ì¥ì†Œ ì¶”ì²œ" in text else "ì¼ë°˜ ìº í•‘"
        return {"classification": classification, "original_question": state["question"]}
    except Exception as e:
        print(f"ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
        return {"classification": "ì¼ë°˜ ìº í•‘", "original_question": state["question"], "error_message": str(e)}

async def generate_general_answer(state: GraphState) -> dict:
    """ì¼ë°˜ ìº í•‘ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    prompt = (
        "ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìº í•‘ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {state['question']}\nì¤„ ë°”ê¿ˆì„ ì´ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ë‹µë³€ í•´ ì£¼ì„¸ìš”.\në‹µë³€:"
    )
    try:
        answer = await oai_text(prompt)
        return {"final_answer": answer}
    except Exception as e:
        print(f"ì¼ë°˜ì§ˆë¬¸ ì˜¤ë¥˜: {e}")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e)}

async def ask_camping_preference(state: GraphState) -> dict:
    """ì‚¬ìš©ìì—ê²Œ ì„ í˜¸í•˜ëŠ” ìº í•‘ ìœ í˜•ì„ ë¬»ëŠ” ë…¸ë“œ"""
    message = (
        "ğŸ•ï¸ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”! ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?\n\n"
        "â–¶ ìœ ë£Œìº í•‘ì¥ (ì˜¤í† ìº í•‘ì¥, í¸ì˜ì‹œì„¤ ì™„ë¹„)\n"
        "â–¶ ê¸€ë¨í•‘/ì¹´ë¼ë°˜ (ëŸ­ì…”ë¦¬, í¸ì•ˆí•œ ìº í•‘)\n"
        "â–¶ ì˜¤ì§€/ë…¸ì§€ìº í•‘ (ìì—° ì† ë¬´ë£Œ ìº í•‘)\n\n"
        "ì¸ì›ì´ë‚˜ ìŠ¤íƒ€ì¼ ë“± ëª¨ë‘ ì•Œë ¤ì£¼ì„¸ìš”!"
    )
    return {"final_answer": message}

async def classify_camping_type(state: GraphState) -> dict:
    """ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ìº í•‘ ìœ í˜•ì„ ë¶„ë¥˜í•˜ëŠ” ë…¸ë“œ"""
    prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ì„ í˜¸ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìì˜ ì‘ë‹µê³¼ ì›ë˜ ì§ˆë¬¸ì„ ì½ê³  'ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘/ì¹´ë¼ë°˜', 'ì˜¤ì§€/ë…¸ì§€ìº í•‘' ì¤‘ í•˜ë‚˜ë¡œ ì •í™•í•˜ê²Œ ì¹´í…Œê³ ë¦¬ë§Œì„ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì›ë˜ ì§ˆë¬¸: {state.get('original_question', '')}\n"
        f"ì‚¬ìš©ì ì‘ë‹µ: {state['question']}\në¶„ë¥˜:"
    )
    try:
        text = await oai_text(prompt)
        categories = ["ìœ ë£Œìº í•‘ì¥", "ê¸€ë¨í•‘/ì¹´ë¼ë°˜", "ì˜¤ì§€/ë…¸ì§€ìº í•‘"]
        camping_type = next((cat for cat in categories if cat in text), "ìœ ë£Œìº í•‘ì¥")
        return {"camping_type_preference": camping_type}
    except Exception as e:
        print(f"ìº í•‘ìœ í˜• ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
        return {"camping_type_preference": "ìœ ë£Œìº í•‘ì¥", "error_message": str(e)}

def _rollup_query():
    """GraphRAG ê²€ìƒ‰ì„ ìœ„í•œ Cypher ì¿¼ë¦¬"""
    return """
    CALL {
      CALL db.index.vector.queryNodes('camp_embedding_index', $topk_camp, $q) YIELD node, score
      WITH node AS camp, score * $w_camp AS s, 'camp' AS src
      WHERE camp.type = $camping_type
      RETURN camp, s, src
      UNION
      CALL db.index.vector.queryNodes('attr_embedding_index', $topk_attr, $q) YIELD node, score
      MATCH (camp:Camp)-[:HAS_ATTRIBUTE]->(node)
      WITH DISTINCT camp, score * $w_attr AS s, 'attr' AS src
      WHERE camp.type = $camping_type
      RETURN camp, s, src
      UNION
      CALL db.index.vector.queryNodes('summary_embedding_index', $topk_sum, $q) YIELD node, score
      MATCH (camp:Camp)-[:HAS_SUMMARY]->(node)
      WITH DISTINCT camp, score * $w_sum AS s, 'summary' AS src
      WHERE camp.type = $camping_type
      RETURN camp, s, src
    }
    WITH camp, collect({src:src, score:s}) AS parts, reduce(total=0.0, x IN collect(s) | total + x) AS totalScore
    ORDER BY totalScore DESC
    LIMIT $rollup_limit
    OPTIONAL MATCH (camp)-[ra:HAS_ATTRIBUTE]->(a:Attribute)
    WITH camp, parts, totalScore, collect({type: ra.type, text: a.text}) AS attributes
    OPTIONAL MATCH (camp)-[:HAS_SUMMARY]->(s:Summary)
    WITH camp, parts, totalScore, attributes, collect(s.text) AS summaries
    RETURN camp, parts, totalScore, attributes, summaries
    ORDER BY totalScore DESC
    """

def _camp_to_location_dict(record):
    """Neo4j ê²€ìƒ‰ ê²°ê³¼ë¥¼ LangChain Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜"""
    camp_node = record["camp"]
    score = float(record.get("totalScore", 0.0))
    camp_props = dict(camp_node)
    
    try:
        camp_meta = json.loads(camp_props.get("meta", "{}"))
        lat = camp_meta.get("ìº í•‘ì¥ ìœ„ë„")
        lon = camp_meta.get("ìº í•‘ì¥ ê²½ë„")
    except (json.JSONDecodeError, AttributeError):
        lat, lon = None, None

    meta_for_doc = {
        "ìº í•‘ì¥ì´ë¦„": camp_props.get("name", ""), "ìš´ì˜ìƒíƒœ": camp_props.get("status"),
        "ìº í•‘ì¥ì£¼ì†Œ": camp_props.get("address"), "ìº í•‘ìœ í˜•": camp_props.get("type"),
        "ìº í•‘ì¥ì‹œì„¤": camp_props.get("facilities"), "ì¦ê¸¸ê±°ë¦¬": camp_props.get("activities"),
        "ìº í•‘ì¥ ìœ„ë„": lat, "ìº í•‘ì¥ ê²½ë„": lon
    }
    
    combined_context = f"## CAMP META\n{json.dumps(meta_for_doc, ensure_ascii=False)}"
    doc = Document(page_content=combined_context, metadata=meta_for_doc)

    return {
        "local_document": {"metadata": meta_for_doc, "content": combined_context, "score": score},
        "doc_for_web": doc, "web_snippet": None
    }

async def search_camping(state: dict, camping_type: str) -> dict:
    """GraphRAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë…¸ë“œ"""
    search_query = (f"ì‚¬ìš©ì ì›ë˜ ì§ˆë¬¸: {state.get('original_question', '')}\n"
                    f"ì‚¬ìš©ì ìº í•‘ ìœ í˜• ë‹µë³€: {state.get('question', '')}").strip()
    task_id = state.get('original_question', f'task_{int(time.time())}')
    print(f"\n[TASK: {task_id}] â¡ï¸ GraphRAG ê²€ìƒ‰ ì‹œì‘ (í•„í„°: {camping_type})")
    
    try:
        # ìƒíƒœ(state)ì—ì„œ embedderë¥¼ ê°€ì ¸ì™€ ì‚¬ìš©
        unified_embedder = state["unified_embedder"]
        query_vecs = await unified_embedder.encode_for_vector_db([search_query], task_id=task_id)
        query_vec = query_vecs[0]

        async with driver.session() as session:
            records = await session.run(
                _rollup_query(), q=query_vec, camping_type=camping_type,
                topk_camp=config.TOPK_CAMP, topk_attr=config.TOPK_ATTR, topk_sum=config.TOPK_SUM,
                w_camp=config.WEIGHT_CAMP, w_attr=config.WEIGHT_ATTR, w_sum=config.WEIGHT_SUM,
                rollup_limit=config.ROLLUP_LIMIT
            )
            rows = [record async for record in records]

        if not rows:
            return {"locations": []}

        locations = [_camp_to_location_dict(rec) for rec in rows[:config.FINAL_TOPN]]
        
        if camping_type != "ì˜¤ì§€/ë…¸ì§€ìº í•‘" and locations:
            docs_with_metadata = [(loc["doc_for_web"], float(loc["local_document"]["score"])) for loc in locations]
            try:
                snippets = await build_snippet_per_doc(docs_with_metadata=docs_with_metadata, per_type_display=20, fetch_timeout=8, max_chars=2000)
                snippet_map = {s["ì¥ì†Œì´ë¦„"]: s for s in snippets}
                for loc in locations:
                    place_name = loc["local_document"]["metadata"].get("ìº í•‘ì¥ì´ë¦„", "")
                    if place_name in snippet_map:
                        loc["web_snippet"] = snippet_map[place_name]
            except Exception as e:
                print(f"[ì›¹ ìŠ¤ë‹ˆí« ì˜¤ë¥˜] {e}")

        for loc in locations:
            loc.pop("doc_for_web", None)

        return {"locations": locations}
    except Exception as e:
        print(f"âŒ [TASK: {task_id}] Neo4j GraphRAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return {"locations": []}

async def search_paid_camping(state): return await search_camping(state, "ìœ ë£Œìº í•‘ì¥")
async def search_glamping_caravan(state): return await search_camping(state, "ê¸€ë¨í•‘/ì¹´ë¼ë°˜")
async def search_ojee_camping(state): return await search_camping(state, "ì˜¤ì§€/ë…¸ì§€ìº í•‘")

async def generate_location_answer(state: GraphState) -> dict:
    """ê²€ìƒ‰ëœ ì¥ì†Œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì¶”ì²œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ"""
    locations = state.get("locations", [])
    context_strs = []
    final_locations = []
    
    for loc in locations:
        local_meta = loc.get("local_document", {}).get("metadata", {})
        snippet = loc.get("web_snippet", {})
        
        location_data = {
            "name": local_meta.get('ìº í•‘ì¥ì´ë¦„'), "address": local_meta.get('ìº í•‘ì¥ì£¼ì†Œ'),
            "latitude": local_meta.get('ìº í•‘ì¥ ìœ„ë„'), "longitude": local_meta.get('ìº í•‘ì¥ ê²½ë„'),
            "local_meta": local_meta,
        }
        final_locations.append(location_data)
        
        context_strs.append(f"ë©”íƒ€ë°ì´í„°: {json.dumps(location_data, ensure_ascii=False)}\në³¸ë¬¸: {loc.get('local_document', {}).get('content', '')}\në„¤ì´ë²„ ì •ë³´: {snippet.get('snippet', '') if snippet else ''}")

    prompt = (
        f"ë‹¹ì‹ ì€ ìº í•‘ì—ì´ì „íŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ '{state.get('camping_type_preference', '')}' ìœ í˜•ì— ë§ëŠ” ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”. ìµœëŒ€í•œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ì¶”ì²œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ ë‚´ ë©”íƒ€ë°ì´í„°ì™€ ìµœì‹  ë„¤ì´ë²„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ì‚¬ì´íŠ¸ë‚˜ ì „í™”ë²ˆí˜¸ë¥¼ ë§í•´ì¤„ë•ŒëŠ” \"ëª¨ë“  ì •ë³´ëŠ” ìµœì‹ ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸/ì „í™”ë¡œ ì¬í™•ì¸ ë°”ëë‹ˆë‹¤.\"ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.\n\n"
        f"ì²« ë²ˆì§¸ ì§ˆë¬¸: {state.get('original_question', '')}\n"
        f"ë‘ ë²ˆì§¸ ì§ˆë¬¸ ë° ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ë‹µë³€: {state.get('question', '')}\n\n"
        f"ë¬¸ë§¥:\n{'---'.join(context_strs)}\n\n"
        "ìœ„ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ì¤„ë°”ê¿ˆì„ ì´ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\në‹µë³€:"
    )
    try:
        answer = await oai_text(prompt)
        return {"final_answer": answer, "locations": final_locations}
    except Exception as e:
        print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e), "locations": final_locations}


# --- 4. LangGraph Workflow Builder ---

def build_workflows():
    """ë‘ ê°œì˜ LangGraph ì›Œí¬í”Œë¡œìš°(ë©”ì¸, í›„ì†)ë¥¼ ë¹Œë“œí•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # 1. ë©”ì¸ ì›Œí¬í”Œë¡œìš°: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ë° ì²« ì‘ë‹µ
    workflow = StateGraph(GraphState)
    workflow.add_node("classify_type", classify_question_type)
    workflow.add_node("generate_general", generate_general_answer)
    workflow.add_node("ask_preference", ask_camping_preference)
    workflow.set_entry_point("classify_type")
    workflow.add_conditional_edges(
        "classify_type",
        lambda state: "general" if state.get("classification") == "ì¼ë°˜ ìº í•‘" else "ask_preference",
        {"general": "generate_general", "ask_preference": "ask_preference"}
    )
    workflow.add_edge("generate_general", END)
    workflow.add_edge("ask_preference", END)
    main_app = workflow.compile()

    # 2. í›„ì† ì›Œí¬í”Œë¡œìš°: ìº í•‘ ìœ í˜•ì— ë”°ë¥¸ ì¥ì†Œ ê²€ìƒ‰ ë° ì¶”ì²œ
    continuation = StateGraph(GraphState)
    continuation.add_node("classify_camping", classify_camping_type)
    continuation.add_node("search_paid", search_paid_camping)
    continuation.add_node("search_glamping", search_glamping_caravan)
    continuation.add_node("search_ojee", search_ojee_camping)
    continuation.add_node("generate_location", generate_location_answer)
    continuation.set_entry_point("classify_camping")
    continuation.add_conditional_edges(
        "classify_camping",
        lambda state: {"ìœ ë£Œìº í•‘ì¥": "paid", "ê¸€ë¨í•‘/ì¹´ë¼ë°˜": "glamping", "ì˜¤ì§€/ë…¸ì§€ìº í•‘": "ojee"}.get(state.get("camping_type_preference", "ìœ ë£Œìº í•‘ì¥")),
        {"paid": "search_paid", "glamping": "search_glamping", "ojee": "search_ojee"}
    )
    continuation.add_edge("search_paid", "generate_location")
    continuation.add_edge("search_glamping", "generate_location")
    continuation.add_edge("search_ojee", "generate_location")
    continuation.add_edge("generate_location", END)
    continuation_app = continuation.compile()

    return main_app, continuation_app