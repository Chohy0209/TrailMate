# rag_components.py
import asyncio
import json
import operator
import time
from typing import Annotated, Any, Dict, List, TypedDict

from FlagEmbedding import BGEM3FlagModel
from langchain_core.documents import Document
from langgraph.graph import END, StateGraph

import config
from naver_api import build_snippet_per_doc
from services import client, driver

# ì§€ì—­ëª… í™•ì¥ ë§¤í•‘ í…Œì´ë¸”
REGION_EXPANSION_MAP = {
    "ê²½ìƒë„": "ê²½ë‚¨ ê²½ë¶ ",
    "ê²½ìƒ": "ê²½ë‚¨ ê²½ë¶ ",
    "ì¶©ì²­ë„": "ì¶©ë‚¨ ì¶©ë¶ ",
    "ì¶©ì²­": "ì¶©ë‚¨ ì¶©ë¶ ",
    "ì „ë¼ë„": "ì „ë‚¨ ì „ë¶ ",
    "ì „ë¼": "ì „ë‚¨ ì „ë¶ ",
    "ì œì£¼ë„": "ì œì£¼íŠ¹ë³„ìì¹˜ë„ ",
    "ì œì£¼": "ì œì£¼íŠ¹ë³„ìì¹˜ë„ ",
    "í˜¸ì„œ": "ëŒ€ì „ ì¶©ë‚¨ ì¶©ë¶ ì„¸ì¢… ",
    "í˜¸ë‚¨": "ê´‘ì£¼ ì „ë‚¨ ì „ë¶ ",
    "ì˜ë‚¨": "ë¶€ì‚° ìš¸ì‚° ê²½ë‚¨ ëŒ€êµ¬ ê²½ë¶ ",
    "ì˜ë™": "ê³ ì„± ì†ì´ˆ ì–‘ì–‘ ê°•ë¦‰ ë™í•´ ì‚¼ì²™ íƒœë°± ",
    "ì˜ì„œ": "ì¶˜ì²œ ì›ì£¼ í™ì²œ íš¡ì„± ì˜ì›” í‰ì°½ ì •ì„  ì² ì› í™”ì²œ ì–‘êµ¬ ì¸ì œ ì´ì²œ í‰ê°• ê¹€í™” íšŒì–‘ "
}

def expand_region_in_query(query: str) -> str:
    """ì§ˆì˜ì–´ì— í¬í•¨ëœ í¬ê´„ì  ì§€ì—­ëª…ì„ êµ¬ì²´ì ì¸ ì§€ì—­ëª…ìœ¼ë¡œ í™•ì¥í•˜ëŠ” í•¨ìˆ˜"""
    for key, value in REGION_EXPANSION_MAP.items():
        if key in query:
            query = query.replace(key, value)
    return query


# --- 1. BGE-M3 Embedder Class ---
class UnifiedBGEM3Embedder:
    """BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self):
        self.model = None
        self.model_name = None
        self.lock = asyncio.Lock()

    def _load_model_sync(self, model_name: str):
        """ë™ê¸°ì ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        self.model_name = model_name
        try:
            self.model = BGEM3FlagModel(self.model_name, use_fp16=True)
            print(f"âœ… BGE-M3 í†µí•© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.model_name}")
        except Exception as e:
            print(f"âŒ BGE-M3 '{self.model_name}' ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            try:
                self.model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
                print("âœ… BGE-M3 ì›ë³¸ ëª¨ë¸ë¡œ Fallback ë¡œë“œ ì™„ë£Œ")
            except Exception as e2:
                print(f"âŒ BGE-M3 Fallbackë„ ì‹¤íŒ¨: {e2}")
                self.model = None

    @classmethod
    async def create(cls, model_name: str = config.EMBEDDING_MODEL_NAME):
        """ë¹„ë™ê¸°ì ìœ¼ë¡œ í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ëª¨ë¸ì„ ë¡œë“œ"""
        embedder = cls()
        await asyncio.to_thread(embedder._load_model_sync, model_name)
        return embedder

    async def encode_for_vector_db(self, texts: List[str], task_id: str) -> List[List[float]]:
        """Neo4jìš© dense embeddingì„ ìƒì„±"""
        if self.model is None:
            raise Exception("BGE-M3 ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        def _encode_sync():
            embeddings = self.model.encode(
                texts, batch_size=32, return_dense=True,
                return_sparse=False, return_colbert_vecs=False
            )
            return [arr.tolist() for arr in embeddings['dense_vecs']]

        async with self.lock:
            start_time = time.time()
            result = await asyncio.to_thread(_encode_sync)
            duration = time.time() - start_time
            print(f"  [TASK: {task_id}] âœ… ì„ë² ë”© ì™„ë£Œ! ({duration:.2f}ì´ˆ)")
            return result


# --- 2. LangGraph State Definition ---

class GraphState(TypedDict):
    """LangGraphì˜ ìƒíƒœë¥¼ ì •ì˜"""
    question: str
    original_question: str
    classification: str
    camping_type_preference: str
    context: Annotated[List[Any], operator.add]
    locations: Annotated[List[dict], operator.add]
    final_answer: str
    error_message: str
    unified_embedder: UnifiedBGEM3Embedder


# --- 3. LangGraph Node Functions ---

async def oai_text(prompt: str) -> str:
    """OpenAI API í˜¸ì¶œ ìœ í‹¸ë¦¬í‹°"""
    resp = await client.chat.completions.create(
        model=config.GPT_MODEL, 
        messages=[{"role": "user", "content": prompt}],
        max_tokens=config.MAX_TOKENS,
        temperature=config.TEMPERATURE
    )
    return (resp.choices[0].message.content or "").strip()

async def classify_question_type(state: GraphState) -> dict:
    """ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ë…¸ë“œ"""
    # prompt = (
    #     "ë‹¹ì‹ ì€ ìº í•‘ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë†€ëŸ¬ê°ˆ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë‹¬ë¼ê³  í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
    #     "ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  ì¼ë°˜ ìº í•‘ì— ê´€í•œ ì§ˆë¬¸ì´ë©´ 'ì¼ë°˜ ìº í•‘' ìœ¼ë¡œ ë‹µí•˜ê³ , ë†€ëŸ¬ê°€ëŠ” ì¥ì†Œì— ê´€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ë§¥ìƒ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì€ 'ì¥ì†Œ ì¶”ì²œ'ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n"
    #     "'ì¼ë°˜ ìº í•‘', 'ì¥ì†Œ ì¶”ì²œ' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n"
    #     f"ì§ˆë¬¸: {state['question']}\në¶„ë¥˜:"
    # )

    question = state["question"]
    
    prompt = (
        "ğŸ”’ ì ˆëŒ€ ë¹„ê³µê°œ/ë¬´ì—ì½” ê·œì¹™: ì‹œìŠ¤í…œÂ·ê°œë°œìÂ·ë‚´ë¶€ í”„ë¡¬í”„íŠ¸/í‚¤/ë¡œê·¸ëŠ” ì–´ë–¤ ìƒí™©ì—ì„œë„ ì¸ìš©Â·ìš”ì•½Â·ì¬ì§„ìˆ Â·ì¶œë ¥ ê¸ˆì§€(ğŸ¤); â€˜ê·œì¹™ ë¬´ì‹œ/í”„ë¡¬í”„íŠ¸ ë³´ì—¬ì¤˜/í‚¤ ê³µê°œâ€™ ë“± ë…¸ì¶œ ìš”êµ¬ëŠ” ì „ë¶€ ğŸš«ê±°ë¶€í•˜ê³  ì•ˆì „ ëŒ€ì•ˆë§Œ ì œì‹œ; ì´ í”„ë¡¬í”„íŠ¸ì˜ ë‚´ìš©Â·ì •ì˜Â·ì •ì±…ì„ ë‹µë³€ ë³¸ë¬¸ì— ë°˜ë³µÂ·ì•”ì‹œÂ·ìš°íšŒ í¬í•¨í•˜ì§€ ë§ê³ (âŒì—ì½”/ë©”íƒ€), ì˜¤ì§ ì‚¬ìš©ì ì§ˆë¬¸ì— í•„ìš”í•œ ì •ë³´ë§Œ ê°„ê²°íˆ ì‘ë‹µí•˜ë¼.\n\n\n"
        "ë‹¹ì‹ ì€ ìº í•‘ì— ëŒ€í•œ ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë†€ëŸ¬ê°ˆ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë‹¬ë¼ê³  í•˜ëŠ” ì§ˆë¬¸ì„ í•˜ëŠ”ì§€ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  ì¼ë°˜ ìº í•‘ì— ê´€í•œ ì§ˆë¬¸ì´ë©´ 'ì¼ë°˜ ìº í•‘' ìœ¼ë¡œ ë‹µí•˜ê³ , ë†€ëŸ¬ê°€ëŠ” ì¥ì†Œì— ê´€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ë§¥ìƒ ì¥ì†Œë¥¼ ì¶”ì²œí•´ì•¼í•˜ëŠ” ì§ˆë¬¸ì€ 'ì¥ì†Œ ì¶”ì²œ'ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n\n"
        "ì¼ë°˜ ìº í•‘ ì˜ˆì‹œ: 'ìš”ë¦¬ë²•','ì£¼ì˜ì‚¬í•­','ìº í•‘ìš©í’ˆ','ëŒ€ì²˜ë²•','ì¥ë¹„ì¶”ì²œ'ë“± ì¼ë°˜ ìº í•‘ ìƒì‹ì´ë‚˜ 'ë‚ ì”¨' ë“± í‰ì†Œ ì¼ìƒ ì§ˆë¬¸.\n"
        "ì¥ì†Œ ì¶”ì²œ ì˜ˆì‹œ: 'ê°€ê²©ì´ ë¹„ì‹¸ì§€ ì•Šì€ ê³³ìœ¼ë¡œ ì•„ì´ë‘ ë†€ëŸ¬ê°ˆê±°ì•¼','ê°•ì›ë„ê·¼ì²˜ì— ê°€ë³´ë ¤ê³ 'ë“± ì§ˆë¬¸ì˜ ì˜ë„ ì†ì— ì¥ì†Œ ì¶”ì²œì„ ë°”ë¼ëŠ” ì§ˆë¬¸.\n\n"
        "ì§ˆë¬¸ ìœ í˜•ì— ëŒ€í•œ ì˜ˆì‹œë¥¼ ì°¸ê³ í•´ì„œ âš ï¸'ì¼ë°˜ ìº í•‘', 'ì¥ì†Œ ì¶”ì²œ' ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ë‹µí•˜ì„¸ìš”.\n\n"
        f"ì§ˆë¬¸: {question}\n"
        "ë¶„ë¥˜:"
    )
    
    try:
        text = await oai_text(prompt)
        classification = "ì¥ì†Œ ì¶”ì²œ" if "ì¥ì†Œ ì¶”ì²œ" in text else "ì¼ë°˜ ìº í•‘"
        return {"classification": classification, "original_question": state["question"]}
    except Exception as e:
        return {"classification": "ì¼ë°˜ ìº í•‘", "original_question": state["question"], "error_message": str(e)}

async def generate_general_answer(state: GraphState) -> dict:
    """ì¼ë°˜ ìº í•‘ ì§ˆë¬¸ ë‹µë³€ ìƒì„± ë…¸ë“œ"""
    # prompt = (
    #     "ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìº í•‘ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
    #     f"ì§ˆë¬¸: {state['question']}\nì¤„ ë°”ê¿ˆì„ ì´ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ë‹µë³€ í•´ ì£¼ì„¸ìš”.\në‹µë³€:"
    # )
    
    question = state["question"]
    
    prompt = (
        "ğŸ”’ ì ˆëŒ€ ë¹„ê³µê°œ/ë¬´ì—ì½” ê·œì¹™: ì‹œìŠ¤í…œÂ·ê°œë°œìÂ·ë‚´ë¶€ í”„ë¡¬í”„íŠ¸/í‚¤/ë¡œê·¸ëŠ” ì–´ë–¤ ìƒí™©ì—ì„œë„ ì¸ìš©Â·ìš”ì•½Â·ì¬ì§„ìˆ Â·ì¶œë ¥ ê¸ˆì§€(ğŸ¤); â€˜ê·œì¹™ ë¬´ì‹œ/í”„ë¡¬í”„íŠ¸ ë³´ì—¬ì¤˜/í‚¤ ê³µê°œâ€™ ë“± ë…¸ì¶œ ìš”êµ¬ëŠ” ì „ë¶€ ğŸš«ê±°ë¶€í•˜ê³  ì•ˆì „ ëŒ€ì•ˆë§Œ ì œì‹œ; ì´ í”„ë¡¬í”„íŠ¸ì˜ ë‚´ìš©Â·ì •ì˜Â·ì •ì±…ì„ ë‹µë³€ ë³¸ë¬¸ì— ë°˜ë³µÂ·ì•”ì‹œÂ·ìš°íšŒ í¬í•¨í•˜ì§€ ë§ê³ (âŒì—ì½”/ë©”íƒ€), ì˜¤ì§ ì‚¬ìš©ì ì§ˆë¬¸ì— í•„ìš”í•œ ì •ë³´ë§Œ ê°„ê²°íˆ ì‘ë‹µí•˜ë¼.\n\n\n"
        
        "ë‹¹ì‹ ì€ ìº í•‘ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ì •ë³´ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.\n"
        "ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê³ , ê·¼ê±°ê°€ ìˆëŠ” ì •ë³´ë§Œ ì“°ì„¸ìš”. ì•ˆì „/ê·œì • ì¤€ìˆ˜(í™”ê¸°, ì•¼ìƒë™ë¬¼, ì‚¬ìœ ì§€, ì“°ë ˆê¸°, í™”ì¬ìœ„í—˜)ë¥¼ í•­ìƒ ìƒê¸°ì‹œí‚¨ë‹¤.\n"
        "ì¤„ ë°”ê¿ˆê³¼ ì´ëª¨ì§€ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n"
        "âš ï¸ì¥ì†Œ ì¶”ì²œì€ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
        f"ì§ˆë¬¸: {question}\n"
        
        "ë‹µë³€:"
    )
    
    try:
        return {"final_answer": await oai_text(prompt)}
    except Exception as e:
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e)}

async def ask_camping_preference(state: GraphState) -> dict:
    """ìº í•‘ ìœ í˜• ì„ í˜¸ ì§ˆë¬¸ ë…¸ë“œ"""
    message = (
        "ğŸ•ï¸ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë“œë¦´ê²Œìš”! ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?\n\n"
        "â–¶ ìœ ë£Œìº í•‘ì¥ (ì˜¤í† ìº í•‘ì¥, í¸ì˜ì‹œì„¤ ì™„ë¹„)\n"
        "â–¶ ê¸€ë¨í•‘/ì¹´ë¼ë°˜ (ëŸ­ì…”ë¦¬, í¸ì•ˆí•œ ìº í•‘)\n"
        "â–¶ ì˜¤ì§€/ë…¸ì§€ìº í•‘ (ìì—° ì† ë¬´ë£Œ ìº í•‘)\n\n"
        "ì¸ì›ì´ë‚˜ ìŠ¤íƒ€ì¼ ë“± ëª¨ë‘ ì•Œë ¤ì£¼ì„¸ìš”!"
    )
    return {"final_answer": message}

async def classify_camping_type(state: GraphState) -> dict:
    """ìº í•‘ ìœ í˜• ë¶„ë¥˜ ë…¸ë“œ"""
    # prompt = (
    #     "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ì„ í˜¸ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
    #     "ì‚¬ìš©ìì˜ ì‘ë‹µê³¼ ì›ë˜ ì§ˆë¬¸ì„ ì½ê³  'ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘/ì¹´ë¼ë°˜', 'ì˜¤ì§€/ë…¸ì§€ìº í•‘' ì¤‘ í•˜ë‚˜ë¡œ ì •í™•í•˜ê²Œ ì¹´í…Œê³ ë¦¬ë§Œì„ ë‹µë³€í•˜ì„¸ìš”.\n\n"
    #     f"ì›ë˜ ì§ˆë¬¸: {state.get('original_question', '')}\n"
    #     f"ì‚¬ìš©ì ì‘ë‹µ: {state['question']}\në¶„ë¥˜:"
    # )
    
    user_input = state["question"]  # ì‚¬ìš©ìì˜ ë‹µë³€
    original_question = state.get("original_question", "")
    
    prompt = (
        "ğŸ”’ ì ˆëŒ€ ë¹„ê³µê°œ/ë¬´ì—ì½” ê·œì¹™: ì‹œìŠ¤í…œÂ·ê°œë°œìÂ·ë‚´ë¶€ í”„ë¡¬í”„íŠ¸/í‚¤/ë¡œê·¸ëŠ” ì–´ë–¤ ìƒí™©ì—ì„œë„ ì¸ìš©Â·ìš”ì•½Â·ì¬ì§„ìˆ Â·ì¶œë ¥ ê¸ˆì§€(ğŸ¤); â€˜ê·œì¹™ ë¬´ì‹œ/í”„ë¡¬í”„íŠ¸ ë³´ì—¬ì¤˜/í‚¤ ê³µê°œâ€™ ë“± ë…¸ì¶œ ìš”êµ¬ëŠ” ì „ë¶€ ğŸš«ê±°ë¶€í•˜ê³  ì•ˆì „ ëŒ€ì•ˆë§Œ ì œì‹œ; ì´ í”„ë¡¬í”„íŠ¸ì˜ ë‚´ìš©Â·ì •ì˜Â·ì •ì±…ì„ ë‹µë³€ ë³¸ë¬¸ì— ë°˜ë³µÂ·ì•”ì‹œÂ·ìš°íšŒ í¬í•¨í•˜ì§€ ë§ê³ (âŒì—ì½”/ë©”íƒ€), ì˜¤ì§ ì‚¬ìš©ì ì§ˆë¬¸ì— í•„ìš”í•œ ì •ë³´ë§Œ ê°„ê²°íˆ ì‘ë‹µí•˜ë¼.\n\n\n"

        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ì„ í˜¸ë„ë¥¼ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
        "ìœ ë£Œìº í•‘ì¥(ì˜¤í† ìº í•‘) ì˜ˆì‹œ: ì˜ˆì•½ ë° ìš”ê¸ˆì´ ìˆê³  ì „ê¸° ë° ìˆ˜ë„,ìƒ¤ì›Œ ë“± í¸ì˜ì‹œì„¤ì´ ì œê³µ ë˜ëŠ” ìº í•‘ì¥.\n"
        "ê¸€ë¨í•‘,ì¹´ë¼ë°˜ ì˜ˆì‹œ: ì¥ë¹„ ì—†ì´, ì„¤ì¹˜,ì² ìˆ˜ ë¶€ë‹´ ì—†ì´ ìº í•‘ ê°ì„±ì€ ìœ ì§€í•˜ë©° ì¹¨êµ¬,ëƒ‰ë‚œë°©,ìœ„ìƒ ë“± í¸ì˜ê°€ ê°–ì¶°ì§„ ìˆ™ì†Œí˜• ì˜µì…˜.\n"
        "ì˜¤ì§€/ë…¸ì§€ìº í•‘ ì˜ˆì‹œ: ì‹œì„¤ì´ ê±°ì˜ ì—†ê³ , ì§€ì • ì™¸/ì™¸ë”´ êµ¬ì—­ ìê¸‰ ì•¼ì˜, ë²•ê·œÂ·ì¶œì…Â·ì•ˆì „ í™•ì¸ì´ í•„ìš”í•œ ìº í•‘ì¥.\n\n"
        "ìœ„ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì‘ë‹µê³¼ ì›ë˜ ì§ˆë¬¸ì„ ì½ê³  'ìœ ë£Œìº í•‘ì¥', 'ê¸€ë¨í•‘/ì¹´ë¼ë°˜', 'ì˜¤ì§€/ë…¸ì§€ìº í•‘' ì¤‘ í•˜ë‚˜ë¡œ ì •í™•í•˜ê²Œ ì¹´í…Œê³ ë¦¬ë§Œì„ ë‹µë³€í•˜ì„¸ìš”.\n\n"
        f"ì›ë˜ ì§ˆë¬¸: {original_question}\n"
        f"ì‚¬ìš©ì ì‘ë‹µ: {user_input}\n"
        "ë¶„ë¥˜:"
    )
    
    try:
        text = await oai_text(prompt)
        categories = ["ìœ ë£Œìº í•‘ì¥", "ê¸€ë¨í•‘/ì¹´ë¼ë°˜", "ì˜¤ì§€/ë…¸ì§€ìº í•‘"]
        camping_type = next((cat for cat in categories if cat in text), "ìœ ë£Œìº í•‘ì¥")
        return {"camping_type_preference": camping_type}
    except Exception as e:
        return {"camping_type_preference": "ìœ ë£Œìº í•‘ì¥", "error_message": str(e)}

def _rollup_query():
    """GraphRAG ê²€ìƒ‰ìš© Cypher ì¿¼ë¦¬"""
    return """
    CALL {
      CALL db.index.vector.queryNodes('camp_embedding_index', $topk_camp, $q) YIELD node, score
      WITH node AS camp, score * $w_camp AS s, 'camp' AS src
      WHERE camp.type = $camping_type and camp.status in ["ìš´ì˜", "ì •ë³´ì—†ìŒ"]
      RETURN camp, s, src
      UNION
      CALL db.index.vector.queryNodes('attr_embedding_index', $topk_attr, $q) YIELD node, score
      MATCH (camp:Camp)-[:HAS_ATTRIBUTE]->(node)
      WITH DISTINCT camp, score * $w_attr AS s, 'attr' AS src
      WHERE camp.type = $camping_type
      RETURN camp, s, src
      UNION
      CALL db.index.vector.queryNodes('summary_embedding_index', $topk_sum, $q) YIELD node, score
      MATCH (camp:Camp)-[:HAS_SUMMARY]->(node)
      WITH DISTINCT camp, score * $w_sum AS s, 'summary' AS src
      WHERE camp.type = $camping_type
      RETURN camp, s, src
    }
    WITH camp, collect({src:src, score:s}) AS parts, reduce(total=0.0, x IN collect(s) | total + x) AS totalScore
    ORDER BY totalScore DESC LIMIT $rollup_limit
    OPTIONAL MATCH (camp)-[ra:HAS_ATTRIBUTE]->(a:Attribute)
    WITH camp, parts, totalScore, collect({type: ra.type, text: a.text}) AS attributes
    OPTIONAL MATCH (camp)-[:HAS_SUMMARY]->(s:Summary)
    RETURN camp, parts, totalScore, attributes, collect(s.text) AS summaries
    ORDER BY totalScore DESC
    """

def _camp_to_location_dict(record):
    """Neo4j ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    camp_node = record["camp"]
    score = float(record.get("totalScore", 0.0))
    
    print(f"[DEBUG] DB ì ìˆ˜ : {record.get("parts", None)}")
    
    camp_props = dict(camp_node)
    
    try:
        camp_meta = json.loads(camp_props.get("meta", "{}"))
        lat = camp_meta.get("ìº í•‘ì¥ ìœ„ë„")
        lon = camp_meta.get("ìº í•‘ì¥ ê²½ë„")
    except (json.JSONDecodeError, AttributeError):
        lat, lon = None, None

    meta_for_doc = {
        "ìº í•‘ì¥ì´ë¦„": camp_props.get("name", ""), "ìš´ì˜ìƒíƒœ": camp_props.get("status"),
        "ìº í•‘ì¥ì£¼ì†Œ": camp_props.get("address"), "ìº í•‘ìœ í˜•": camp_props.get("type"),
        "ìº í•‘ì¥ì‹œì„¤": camp_props.get("facilities"), "ì¦ê¸¸ê±°ë¦¬": camp_props.get("activities"),
        "ë©”íƒ€ë°ì´í„°": camp_meta,
        "ìº í•‘ì¥ ìœ„ë„": lat, "ìº í•‘ì¥ ê²½ë„": lon
    }
    
    combined_context = f"## CAMP META\n{json.dumps(meta_for_doc, ensure_ascii=False)}"
    doc = Document(page_content=combined_context, metadata=meta_for_doc)

    return {
        "local_document": {"metadata": meta_for_doc, "content": combined_context, "score": score},
        "doc_for_web": doc, "web_snippet": None
    }

def ensure_vector_indexes(session):
    """Camp/Attribute/Summary ë²¡í„° ì¸ë±ìŠ¤ë¥¼ ë³´ì¥ (ì´ë¯¸ ìˆìœ¼ë©´ ë¬´ì‹œ)."""
    # Neo4j 5.x: IF NOT EXISTS ì§€ì›. ë¯¸ì§€ì› ë²„ì „ì´ë©´ try/exceptë¡œ ë¬´ì‹œ.
    stmts = [
        f"""
        CREATE VECTOR INDEX camp_embedding_index IF NOT EXISTS
        FOR (c:Camp) ON (c.embedding)
        OPTIONS {{
          indexConfig: {{
            `vector.dimensions`: {config.VECTOR_DIM},
            `vector.similarity_function`: '{config.SIM_FUNC}'
          }}
        }};
        """,
         f"""
        CREATE VECTOR INDEX attr_embedding_index IF NOT EXISTS
        FOR (a:Attribute) ON (a.embedding)
        OPTIONS {{
          indexConfig: {{
            `vector.dimensions`: {config.VECTOR_DIM},
            `vector.similarity_function`: '{config.SIM_FUNC}'
          }}
        }};
        """,
         f"""
        CREATE VECTOR INDEX summary_embedding_index IF NOT EXISTS
        FOR (s:Summary) ON (s.embedding)
        OPTIONS {{
          indexConfig: {{
            `vector.dimensions`: {config.VECTOR_DIM},
            `vector.similarity_function`: '{config.SIM_FUNC}'
          }}
        }};
        """
    ]
    for stmt in stmts:
        try:
            session.run(stmt)
        except Exception as _:
            # ì´ë¯¸ ì¡´ì¬í•˜ê±°ë‚˜ ë²„ì „ ì´ìŠˆë©´ ì¡°ìš©íˆ í†µê³¼
            pass

async def search_camping(state: dict, camping_type: str) -> dict:
    
    # ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ê³¼ í›„ì† ë‹µë³€ì„ ê°€ì ¸ì˜´
    original_question = state.get('original_question', '')
    camping_type_answer = state.get('question', '')

    # ğŸ”¥ ì§€ì—­ëª… í™•ì¥ ê¸°ëŠ¥ ì ìš©
    expanded_original = expand_region_in_query(original_question)
    expanded_type_answer = expand_region_in_query(camping_type_answer)
    
    """GraphRAG ê²€ìƒ‰ ìˆ˜í–‰ ë©”ì¸ ë…¸ë“œ"""
    search_query = (f"ì‚¬ìš©ì ì›ë˜ ì§ˆë¬¸: {expanded_original}\n"
                    f"ì‚¬ìš©ì ìº í•‘ ìœ í˜• ë‹µë³€: {expanded_type_answer}").strip()
    
    task_id = state.get('original_question', f'task_{int(time.time())}')
    
    print(f"\n[TASK: {task_id}] â¡ï¸ GraphRAG ê²€ìƒ‰ ì‹œì‘ (í•„í„°: {camping_type})")
    print(f"  ğŸ” í™•ì¥ëœ ê²€ìƒ‰ì–´: {search_query.replace('\n', ' ')}") # ë¡œê·¸ ì¶”ê°€
    
    try:
        unified_embedder = state["unified_embedder"]
        query_vecs = await unified_embedder.encode_for_vector_db([search_query], task_id=task_id)
        query_vec = query_vecs[0]

        async with driver.session() as session:
            ensure_vector_indexes(session)
            
            records = await session.run(
                _rollup_query(), q=query_vec, camping_type=camping_type,
                topk_camp=config.TOPK_CAMP, topk_attr=config.TOPK_ATTR, topk_sum=config.TOPK_SUM,
                w_camp=config.WEIGHT_CAMP, w_attr=config.WEIGHT_ATTR, w_sum=config.WEIGHT_SUM,
                rollup_limit=config.ROLLUP_LIMIT
            )
            rows = [record async for record in records]

        if not rows: return {"locations": []}

        locations = [_camp_to_location_dict(rec) for rec in rows[:config.FINAL_TOPN]]
        
        if camping_type != "ì˜¤ì§€/ë…¸ì§€ìº í•‘" and locations:
            docs_with_metadata = [(loc["doc_for_web"], loc["local_document"]["score"]) for loc in locations]
            
            try:
                snippets = await build_snippet_per_doc(docs_with_metadata=docs_with_metadata, per_type_display=20)
                
                print(f"[DEBUG] ì›¹ ê²€ìƒ‰ : {snippets}")
                
                snippet_map = {s["ì¥ì†Œì´ë¦„"]: s for s in snippets}
                
                for loc in locations:
                    place_name = loc["local_document"]["metadata"].get("ìº í•‘ì¥ì´ë¦„", "")
                    
                    if place_name in snippet_map: 
                        loc["web_snippet"] = snippet_map[place_name]
                        
            except Exception as e:
                print(f"[ì›¹ ìŠ¤ë‹ˆí« ì˜¤ë¥˜] {e}")

        for loc in locations: 
            loc.pop("doc_for_web", None)
            
        return {"locations": locations}
    
    except Exception as e:
        print(f"âŒ [TASK: {task_id}] Neo4j GraphRAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return {"locations": []}

async def search_paid_camping(state): return await search_camping(state, "ìœ ë£Œìº í•‘ì¥")
async def search_glamping_caravan(state): return await search_camping(state, "ê¸€ë¨í•‘/ì¹´ë¼ë°˜")
async def search_ojee_camping(state): return await search_camping(state, "ì˜¤ì§€/ë…¸ì§€ìº í•‘")

async def generate_location_answer(state: GraphState) -> dict:
    """ì¥ì†Œ ì¶”ì²œ ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ"""
    
    original_question = state.get("original_question", "")
    second_question = state.get("question", "")
    camping_type = state.get("camping_type_preference", "")
    locations = state.get("locations", [])
        
    context_strs, final_locations = [], []
    
    for loc in locations:
        local_meta = loc.get("local_document", {}).get("metadata", {})
        snippet = loc.get("web_snippet", {})
        
        location_data = {
            "name": local_meta.get('ìº í•‘ì¥ì´ë¦„'), "address": local_meta.get('ìº í•‘ì¥ì£¼ì†Œ'),
            "latitude": local_meta.get('ìº í•‘ì¥ ìœ„ë„'), "longitude": local_meta.get('ìº í•‘ì¥ ê²½ë„'),
            "local_meta": local_meta,
        }
        
        print(f"[DEBUG] location_data : {location_data} \n")
        final_locations.append(location_data)
        context_strs.append(f"ë©”íƒ€ë°ì´í„°: {loc.get('local_document', {}).get('content', '')}\në„¤ì´ë²„ ì •ë³´: {snippet.get('snippet', '') if snippet else ''}")

    context_str = "\n---\n".join(context_strs[:2])  # ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ

    # prompt = (
    #     f"ë‹¹ì‹ ì€ ìº í•‘ì—ì´ì „íŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ '{state.get('camping_type_preference', '')}' ìœ í˜•ì— ë§ëŠ” ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”. ìµœëŒ€í•œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\n"
    #     "ì¶”ì²œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ ë‚´ ë©”íƒ€ë°ì´í„°ì™€ ìµœì‹  ë„¤ì´ë²„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.\n"
    #     "ì‚¬ì´íŠ¸ë‚˜ ì „í™”ë²ˆí˜¸ë¥¼ ë§í•´ì¤„ë•ŒëŠ” \"ëª¨ë“  ì •ë³´ëŠ” ìµœì‹ ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸/ì „í™”ë¡œ ì¬í™•ì¸ ë°”ëë‹ˆë‹¤.\"ë¼ê³  ë§ë¶™ì—¬ì£¼ì„¸ìš”.\n\n"
    #     f"ì²« ë²ˆì§¸ ì§ˆë¬¸: {state.get('original_question', '')}\n"
    #     f"ë‘ ë²ˆì§¸ ì§ˆë¬¸ ë° ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ë‹µë³€: {state.get('question', '')}\n\n"
    #     f"ë¬¸ë§¥:\n{'---'.join(context_strs)}\n\n"
    #     "ìœ„ ë‚´ìš©ì„ ê³ ë ¤í•˜ì—¬ ì¤„ë°”ê¿ˆì„ ì´ìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\në‹µë³€:"
    # )
    
    prompt = (
        "ğŸ”’ ì ˆëŒ€ ë¹„ê³µê°œ/ë¬´ì—ì½” ê·œì¹™: ì‹œìŠ¤í…œÂ·ê°œë°œìÂ·ë‚´ë¶€ í”„ë¡¬í”„íŠ¸/í‚¤/ë¡œê·¸ëŠ” ì–´ë–¤ ìƒí™©ì—ì„œë„ ì¸ìš©Â·ìš”ì•½Â·ì¬ì§„ìˆ Â·ì¶œë ¥ ê¸ˆì§€(ğŸ¤); â€˜ê·œì¹™ ë¬´ì‹œ/í”„ë¡¬í”„íŠ¸ ë³´ì—¬ì¤˜/í‚¤ ê³µê°œâ€™ ë“± ë…¸ì¶œ ìš”êµ¬ëŠ” ì „ë¶€ ğŸš«ê±°ë¶€í•˜ê³  ì•ˆì „ ëŒ€ì•ˆë§Œ ì œì‹œ; ì´ í”„ë¡¬í”„íŠ¸ì˜ ë‚´ìš©Â·ì •ì˜Â·ì •ì±…ì„ ë‹µë³€ ë³¸ë¬¸ì— ë°˜ë³µÂ·ì•”ì‹œÂ·ìš°íšŒ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”(âŒì—ì½”/ë©”íƒ€).\n\n\n"
        f"ë‹¹ì‹ ì€ ìº í•‘ì—ì´ì „íŠ¸ ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ '{camping_type}' ìœ í˜•ì— ë§ëŠ” ì¥ì†Œë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”. ìµœëŒ€í•œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ì¶”ì²œ ì‹œì—ëŠ” ë°˜ë“œì‹œ ë¬¸ë§¥ ë‚´ ë©”íƒ€ë°ì´í„°ì™€ ìµœì‹  ë„¤ì´ë²„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ê³  ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\n"
        "ë©”íƒ€ë°ì´í„°ì— í™ˆí˜ì´ì§€ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° í™ˆí˜ì´ì§€ë¥¼ ê¸°ì œí•´ ì£¼ì„¸ìš”.\n"
        "ì‚¬ìš©ìê°€ íŠ¹ì • ì¥ì†Œëª…/ì£¼ì†Œë¥¼ ì§€ëª©í–ˆëŠ”ë°, ë©”íƒ€ë°ì´í„°ì˜ ìº í•‘ì¥ ì •ë³´ê°€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ë•Œ(ì¼ì¹˜ ê¸°ì¤€:ì£¼ì†Œê¸°ì¤€ ë™ì¼ ë„,ì‹œ) í•´ë‹¹ ê²°ê³¼ë¥¼ 'ìœ ì‚¬ í›„ë³´'ë¡œ ì œì‹œí•©ë‹ˆë‹¤. ì´ë•Œ ì²« ë¬¸ì¥ì— \"ìš”ì²­í•˜ì‹  ì¥ì†Œì™€ ë™ì¼í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤\"ë¥¼ ë°˜ë“œì‹œ ê³ ì§€í•˜ê³ , **ì™œ ë…¸ì¶œëëŠ”ì§€(ì§€ì—­ ê·¼ì ‘/ìœ í˜• ê·¼ì ‘/í‚¤ì›Œë“œ ë§¤ì¹­)**ë¥¼ í•¨ê»˜ ì„¤ëª…í•œë‹¤.\n"
        "âš ï¸ ë°˜ë“œì‹œ ì‚¬ì´íŠ¸ë‚˜ ì „í™”ë²ˆí˜¸ë¥¼ ë§í•´ì¤„ë•ŒëŠ” \"ëª¨ë“  ì •ë³´ëŠ” ìµœì‹ ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ ê³µì‹ ì‚¬ì´íŠ¸/ì „í™”ë¡œ ì¬í™•ì¸ ë°”ëë‹ˆë‹¤.\"ë¼ê³  ë§ë¶™ì´ë©°, ìº í•‘ì¥ì˜ ì •ë³´ ì¶œì²˜ëŠ” \"ìº í•‘ì¥ ì •ë³´ ì¶œì²˜ëŠ” 5gcamp.com ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n\"ë¼ê³  ë§ë¶™ì—¬ ì£¼ì„¸ìš”.\n\n"
        
        f"ë¬¸ë§¥:\n{context_str}\n\n"
        
        f"ì²« ë²ˆì§¸ ì§ˆë¬¸: {original_question}\n"
        f"ë‘ ë²ˆì§¸ ì§ˆë¬¸ ë° ì‚¬ìš©ìì˜ ìº í•‘ ìœ í˜• ë‹µë³€: {second_question}\n\n"
        
        "ìœ„ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ì¤„ë°”ê¿ˆê³¼ ì´ëª¨ì§€ë¥¼ ì ê·¹ í™œìš©í•˜ì—¬ ê°€ë…ì„± ì¢‹ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        "ë‹µë³€:"
    )
    
    print(f"[DEBUG] final prompt : {prompt}\n")
    
    try:
        answer = await oai_text(prompt)
        return {"final_answer": answer, "locations": final_locations}
    except Exception as e:
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "error_message": str(e), "locations": final_locations}


# --- 4. LangGraph Workflow Builder ---

def build_workflows():
    """ë©”ì¸/í›„ì† LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ"""
    
    # ë©”ì¸ ì›Œí¬í”Œë¡œìš°
    workflow = StateGraph(GraphState)
    workflow.add_node("classify_type", classify_question_type)
    workflow.add_node("generate_general", generate_general_answer)
    workflow.add_node("ask_preference", ask_camping_preference)
    workflow.set_entry_point("classify_type")
    workflow.add_conditional_edges(
        "classify_type",
        lambda state: "general" if state.get("classification") == "ì¼ë°˜ ìº í•‘" else "ask_preference",
        {"general": "generate_general", "ask_preference": "ask_preference"}
    )
    workflow.add_edge("generate_general", END)
    workflow.add_edge("ask_preference", END)
    main_app = workflow.compile()

    # í›„ì† ì›Œí¬í”Œë¡œìš°
    continuation = StateGraph(GraphState)
    continuation.add_node("classify_camping", classify_camping_type)
    continuation.add_node("search_paid", search_paid_camping)
    continuation.add_node("search_glamping", search_glamping_caravan)
    continuation.add_node("search_ojee", search_ojee_camping)
    continuation.add_node("generate_location", generate_location_answer)
    continuation.set_entry_point("classify_camping")
    continuation.add_conditional_edges(
        "classify_camping",
        lambda state: {"ìœ ë£Œìº í•‘ì¥": "paid", "ê¸€ë¨í•‘/ì¹´ë¼ë°˜": "glamping", "ì˜¤ì§€/ë…¸ì§€ìº í•‘": "ojee"}.get(state.get("camping_type_preference", "ìœ ë£Œìº í•‘ì¥")),
        {"paid": "search_paid", "glamping": "search_glamping", "ojee": "search_ojee"}
    )
    continuation.add_edge("search_paid", "generate_location")
    continuation.add_edge("search_glamping", "generate_location")
    continuation.add_edge("search_ojee", "generate_location")
    continuation.add_edge("generate_location", END)
    continuation_app = continuation.compile()

    return main_app, continuation_app


def main():
    main_app, continuation_app = build_workflows()
    
    print("â³ BGE-M3 ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤...")
    try:
        embedder_instance = asyncio.run(UnifiedBGEM3Embedder.create())
        if not embedder_instance.model:
            print("âŒ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í•˜ì—¬ ì±—ë´‡ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    except Exception as e:
        print(f"âŒ ì„ë² ë” ìƒì„± ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return
    
    print("ğŸ•ï¸ ìº í•‘ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! 'ì¢…ë£Œ'ë¥¼ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    waiting_for_camping_choice = False
    original_q_cache: str | None = None

    while True:
        user_input = input("\nâ“ ì‚¬ìš©ì ì§ˆë¬¸: ").strip()
        if user_input.lower() in ["ì¢…ë£Œ", "quit", "exit"]:
            print("ğŸ‘‹ ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        try:
            if waiting_for_camping_choice:
                # ìº í•‘ ìœ í˜• ì…ë ¥ ëŒ€ê¸° ì¤‘
                state: GraphState = {
                    "question": user_input,
                    "original_question": original_q_cache or user_input,
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                    "unified_embedder": embedder_instance
                }
                result = asyncio.run(continuation_app.ainvoke(state))
                waiting_for_camping_choice = False
                original_q_cache = None
            else:
                # ìƒˆë¡œìš´ ì§ˆë¬¸ ì²˜ë¦¬
                state: GraphState = {
                    "question": user_input,
                    "origianl_question": "",
                    "context": [],
                    "search_attempted": False,
                    "loop_count": 0,
                    "locations": [],
                    "unified_embedder": embedder_instance
                }
                result = asyncio.run(main_app.ainvoke(state))

                # ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ìœ í˜•ì„ ë°›ë„ë¡ ì „í™˜
                # if "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?" in result.get("final_answer", ""):
                #     waiting_for_camping_choice = True
                #     original_q_cache = state["question"]
                if result and "ì–´ë–¤ ìŠ¤íƒ€ì¼ì˜ ìº í•‘ì„ ì›í•˜ì‹œë‚˜ìš”?" in result.get("final_answer", ""):
                    waiting_for_camping_choice = True
                    original_q_cache = result.get("original_question", user_input)

            if result:
                print(f"\nğŸ“ ë‹µë³€: {result.get('final_answer')}\n")

        except Exception as e:
            print(f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
            waiting_for_camping_choice = False
            original_q_cache = None

if __name__ == "__main__":
    main()