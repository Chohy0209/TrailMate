import operator
from typing import TypedDict, Annotated, List, Any
import asyncio
import requests
import time
import random
import torch

# LangChain ë° Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langgraph.graph import StateGraph, END, START
from langchain.schema import Document

# ==============================================================================
# 1. ì„¤ì • ë° ìƒìˆ˜
# ==============================================================================
# LangGraph ë£¨í”„ ìµœëŒ€ íšŸìˆ˜ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
MAX_LOOPS = 2
# LLM í˜¸ì¶œ ì‹œ íƒ€ì„ì•„ì›ƒ (ì´ˆ ë‹¨ìœ„)
LLM_TIMEOUT = 30
# ëª¨ë¸ ë° DB ê²½ë¡œ
MODEL_DIR = "merged_model"
CLASSIFIER_MODEL_NAME = "merged_model"
EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large-instruct"
CHROMA_DB_PATH = './data/camp_chroma_store'

# ==============================================================================
# 2. ìƒíƒœ ì •ì˜ (LangGraph State)
# ==============================================================================
class GraphState(TypedDict):
    """
    LangGraph ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” TypedDict
    """
    question: str
    classification: str
    context: Annotated[List[Any], operator.add]
    locations: Annotated[List[dict], operator.add]
    final_answer: str
    route_decision: str
    loop_count: int
    search_attempted: bool
    error_message: str

# ==============================================================================
# 3. ëª¨ë¸ ë° ë²¡í„° DB ë¡œë”©
# ==============================================================================
print("--- LLM ëª¨ë¸ ë¡œë”© ì¤‘ ---")
start_time = time.perf_counter()

# ë©”ì¸ LLM ëª¨ë¸
model_main = AutoModelForCausalLM.from_pretrained(MODEL_DIR, device_map="auto", torch_dtype=torch.float16)
tokenizer_main = AutoTokenizer.from_pretrained(MODEL_DIR)
if tokenizer_main.pad_token is None:
    tokenizer_main.pad_token = tokenizer_main.eos_token
llm_pipeline_main = pipeline("text-generation", model=model_main, tokenizer=tokenizer_main, max_new_tokens=1024, do_sample=True, temperature=0.7, pad_token_id=tokenizer_main.eos_token_id)
llm_main = HuggingFacePipeline(pipeline=llm_pipeline_main)

# ë¶„ë¥˜ìš© ê²½ëŸ‰ ëª¨ë¸
model_classifier = AutoModelForCausalLM.from_pretrained(CLASSIFIER_MODEL_NAME, device_map="auto", torch_dtype=torch.float16)
tokenizer_classifier = AutoTokenizer.from_pretrained(CLASSIFIER_MODEL_NAME)
if tokenizer_classifier.pad_token is None:
    tokenizer_classifier.pad_token = tokenizer_classifier.eos_token
llm_pipeline_classifier = pipeline("text-generation", model=model_classifier, tokenizer=tokenizer_classifier, max_new_tokens=100, do_sample=True, temperature=0.3, pad_token_id=tokenizer_classifier.eos_token_id)
llm_classifier = HuggingFacePipeline(pipeline=llm_pipeline_classifier)

end_time = time.perf_counter()
print(f"--- ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")

print("--- ì„ë² ë”© ëª¨ë¸ ë° ChromaDB ë¡œë”© ì¤‘ ---")
start_time = time.perf_counter()
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
vectordb = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
retriever = vectordb.as_retriever(search_kwargs={"k": 2})
end_time = time.perf_counter()
print(f"--- ì„ë² ë”© ëª¨ë¸ ë° ChromaDB ë¡œë”© ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")

# ==============================================================================
# 4. í—¬í¼ í•¨ìˆ˜
# ==============================================================================
def extract_llm_response(raw_response: str, prompt: str) -> str:
    """LLM ì‘ë‹µì—ì„œ ì‹¤ì œ ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
    response_part = raw_response.split(prompt)[-1].strip()
    first_line = response_part.split('\n')[0].strip()
    return first_line

# ==============================================================================
# 5. LangGraph ë…¸ë“œ í•¨ìˆ˜ (Graph Nodes)
# ==============================================================================
def primary_classify_question(state: GraphState) -> dict:
    """
    ì§ˆë¬¸ì„ 'ì¼ë°˜ ìº í•‘' ë˜ëŠ” 'ì¥ì†Œ ì¶”ì²œ'ìœ¼ë¡œ 1ì°¨ ë¶„ë¥˜
    """
    print("--- 1ì°¨ ë¶„ë¥˜ ì¤‘ (ì¼ë°˜ vs ì¥ì†Œ) ---")
    question = state['question']
    prompt = f"""ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ 1ì°¨ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³  'ì¼ë°˜ ìº í•‘' ë˜ëŠ” 'ì¥ì†Œ ì¶”ì²œ' ì¤‘ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.

<ë¶„ë¥˜ ê¸°ì¤€>
- ì¼ë°˜ ìº í•‘: ìº í•‘ ì¤€ë¹„ë¬¼, ì¥ë¹„, íŒ, ë°©ë²• ë“± ìº í•‘ ìì²´ì— ëŒ€í•œ ì§ˆë¬¸
- ì¥ì†Œ ì¶”ì²œ: íŠ¹ì • ì§€ì—­ì˜ ìº í•‘ ì¥ì†Œë¥¼ ì¶”ì²œí•´ë‹¬ë¼ëŠ” ì§ˆë¬¸

ì§ˆë¬¸: {question}
ë¶„ë¥˜:"""
    try:
        raw_response = llm_classifier.invoke(prompt)
        classification = extract_llm_response(raw_response, prompt)
        
        if classification not in ['ì¼ë°˜ ìº í•‘', 'ì¥ì†Œ ì¶”ì²œ']:
            classification = 'ì¼ë°˜ ìº í•‘'
            print("âš ï¸ 1ì°¨ ë¶„ë¥˜ ê²°ê³¼ê°€ ë¶ˆë¶„ëª…í•˜ì—¬ 'ì¼ë°˜ ìº í•‘'ìœ¼ë¡œ ê¸°ë³¸ ì²˜ë¦¬")

        print(f"1ì°¨ ë¶„ë¥˜ LLM ì‘ë‹µ: {classification}")
        return {"classification": classification}

    except Exception as e:
        print(f"1ì°¨ ë¶„ë¥˜ ì˜¤ë¥˜: {e}. 'ì¼ë°˜ ìº í•‘'ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        return {"classification": "ì¼ë°˜ ìº í•‘", "error_message": str(e)}

async def retrieve_context_async(state: GraphState) -> dict:
    """
    'ì¥ì†Œ ì¶”ì²œ' ì§ˆë¬¸ì— ëŒ€í•´ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë¬¸ë§¥(context)ê³¼ ìœ„ì¹˜ ì •ë³´ë¥¼ ê²€ìƒ‰
    """
    print("--- ì¥ì†Œ ì¶”ì²œ ì§ˆë¬¸ì— ëŒ€í•´ ì •ë³´ ê²€ìƒ‰ ì‹œì‘ ---")
    question = state['question']
    locations = []  # ìœ„ì¹˜ ì •ë³´ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    try:
        # 'ì¥ì†Œ ì¶”ì²œ' ì§ˆë¬¸ì€ ì¼ë‹¨ RAG ê²€ìƒ‰ì„ ë¨¼ì € ìˆ˜í–‰
        docs = await asyncio.to_thread(retriever.get_relevant_documents, question)
        context = [f"[ë²¡í„°DB] {doc.page_content}" for doc in docs]

        # ë©”íƒ€ë°ì´í„°ì—ì„œ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
        for doc in docs:
            if 'ìœ„ë„' in doc.metadata and 'ê²½ë„' in doc.metadata:
                # ìº í•‘ì¥ ì´ë¦„ì€ page_contentì˜ ì²« ì¤„ë¡œ ê°€ì •
                name = doc.page_content.split('\n')[0].strip()
                locations.append({
                    "name": name,
                    "lat": doc.metadata['ìœ„ë„'],
                    "lon": doc.metadata['ê²½ë„']
                })
        
        if locations:
            print(f"     -> {len(locations)}ê°œì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

        # RAG ê²€ìƒ‰ ê²°ê³¼ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ í™•ì¸í•˜ì—¬ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œì§€ íŒë‹¨
        requires_web_search = False
        for doc in docs:
            # ë©”íƒ€ë°ì´í„°ì— 'ìœ ë£Œ' ë˜ëŠ” 'ê¸€ë¨í•‘', 'ì¹´ë¼ë°˜' ë“±ì˜ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if 'ìœ ë£Œ' in doc.metadata.get('type', '') or \
               'ê¸€ë¨í•‘' in doc.page_content or \
               'ì¹´ë¼ë°˜' in doc.page_content:
                requires_web_search = True
                break
        
        if requires_web_search:
            print("     -> ê²€ìƒ‰ ê²°ê³¼ì— ìœ ë£Œ/ì‹œì„¤ ê´€ë ¨ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì–´ ì›¹ ê²€ìƒ‰ì„ ì¶”ê°€ ì§„í–‰í•©ë‹ˆë‹¤.")
            web_results = await web_search_real_async(question)
            context.extend(web_results)

        print(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {len(context)}ê°œ")
        return {"context": context, "locations": locations, "search_attempted": True}
    
    except Exception as e:
        print(f"ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return {"context": ["ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."], "locations": [], "error_message": str(e), "search_attempted": True}

async def web_search_real_async(query: str) -> List[str]:
    """ìœ ë£Œ ìº í•‘ì¥ ì „ìš© ì›¹ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ (ë”ë¯¸ ë°ì´í„°)ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
    print(f"     -> ì›¹ ê²€ìƒ‰ ì¤‘: '{query}'")
    await asyncio.sleep(0.5) # ì‹¤ì œ API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
    paid_camping_data = [
        "[ì›¹ê²€ìƒ‰] ê°€í‰ ë³„ë¹›ê¸€ë¨í•‘íŒŒí¬ - ì˜ˆì•½ ê°€ëŠ¥, 1ë°• 15ë§Œì›, ë°”ë² íì‹œì„¤ ì™„ë¹„",
        "[ì›¹ê²€ìƒ‰] ì¶˜ì²œ ë‚¨ì´ì„¬ê¸€ë¨í•‘ - ì£¼ë§ ì˜ˆì•½ ë§ˆê°, í‰ì¼ ì˜ˆì•½ ê°€ëŠ¥, í˜¸ìˆ˜ë·°",
        "[ì›¹ê²€ìƒ‰] ì–‘í‰ ë“¤ê½ƒìˆ˜ëª©ì› ì˜¤í† ìº í•‘ì¥ - ì „ê¸°/ìˆ˜ë„ ì‹œì„¤, 1ë°• 3ë§Œì›",
    ]
    regional_keywords = {"ê°•ì›": ["ì¶˜ì²œ"], "ê²½ê¸°": ["ê°€í‰", "ì–‘í‰"]}
    region_specific = []
    if any(city in query for city in regional_keywords.get("ê²½ê¸°", [])):
        region_specific.extend(["[ì›¹ê²€ìƒ‰] ê²½ê¸° ì§€ì—­ ì¸ê¸° ê¸€ë¨í•‘ì¥ TOP 5"])
    
    all_data = paid_camping_data + region_specific
    num_results = random.randint(2, 4)
    selected_results = random.sample(all_data, min(num_results, len(all_data)))
    
    print(f"     -> ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(selected_results)}ê°œ ê²°ê³¼")
    return selected_results

def generate_final_answer(state: GraphState) -> dict:
    """
    ì£¼ì–´ì§„ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± (ë¬¸ë§¥ì´ ì—†ìœ¼ë©´ ìì²´ ìƒì„±)
    """
    print("--- ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ---")
    start_time = time.perf_counter()
    
    question = state['question']
    context = state.get('context', [])
    
    context_str = ""
    if context:
        context_str = "\n".join([ctx.replace("[ì›¹ê²€ìƒ‰]", "").replace("[ë²¡í„°DB]", "").strip() for ctx in context[:5]])
    
    prompt = f"""ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ëŠ¥ìˆ™í•œ ìº í•‘ ì „ë¬¸ê°€ ì±—ë´‡ ì…ë‹ˆë‹¤.
{'ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì„ ë°”íƒ•ìœ¼ë¡œ' if context else ''} ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë§Œì•½ ì£¼ì–´ì§„ ë¬¸ë§¥ë§Œìœ¼ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ì •ë³´ë¡œëŠ” ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."ë¼ê³  ë§í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ êµ¬ì²´ì ì´ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

{f"ë¬¸ë§¥: {context_str}" if context_str else ""}
ì§ˆë¬¸: {question}

ë‹µë³€:"""

    try:
        response = llm_pipeline_main(prompt, 
        max_new_tokens=512, 
        do_sample=True, 
        temperature=0.6,
        repetition_penalty=1.2,

        pad_token_id=tokenizer_main.eos_token_id, 
        eos_token_id=tokenizer_main.eos_token_id)
        final_answer = response[0]['generated_text'].replace(prompt, "").strip().replace("ë‹µë³€:", "").strip()
        
        if not final_answer or len(final_answer.strip()) < 10:
            if context_str:
                final_answer = f"""ì£„ì†¡í•©ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•´ ì£¼ì„¸ìš”:\n{context_str[:500]}"""
            else:
                 final_answer = "ì£„ì†¡í•©ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        end_time = time.perf_counter()
        print(f"--- ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ (ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ) ---")
        return {"final_answer": final_answer, "loop_count": state.get('loop_count', 0) + 1}

    except Exception as e:
        print(f"ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {e}")
        return {"final_answer": "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "error_message": str(e), "loop_count": state.get('loop_count', 0) + 1}


def check_answer_quality(state: GraphState) -> dict:
    """
    ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³ , í•„ìš”ì‹œ ì¶”ê°€ ê²€ìƒ‰ì„ ê²°ì •
    """
    print("--- ë‹µë³€ í’ˆì§ˆ ê²€ì¦ ì¤‘ ---")
    final_answer = state['final_answer']
    context = state.get('context', [])
    current_loop = state.get('loop_count', 0)
    
    # RAGê°€ í•„ìš”í•œ ì§ˆë¬¸ì—ë§Œ ì¬ì‹œë„ ë¡œì§ ì ìš©
    if state['classification'] == 'ì¥ì†Œ ì¶”ì²œ':
        insufficient_indicators = ["ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜", "ì œê³µëœ ì •ë³´ê°€ ë¶€ì¡±", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤"]
        is_insufficient = any(indicator in final_answer for indicator in insufficient_indicators)
        
        if is_insufficient and current_loop < MAX_LOOPS:
            print(f"     -> ë‹µë³€ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. (í˜„ì¬ ë£¨í”„: {current_loop}/{MAX_LOOPS}) ì¶”ê°€ ê²€ìƒ‰ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            return {"route_decision": "continue"}
        else:
            print("     -> ë‹µë³€ì„ ì™„ë£Œí•˜ê±°ë‚˜ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
            return {"route_decision": "end"}
    else:
        # ì¼ë°˜ ìº í•‘ ì§ˆë¬¸ì€ ì¬ì‹œë„ ì—†ì´ ë°”ë¡œ ì¢…ë£Œ
        print("     -> ì¼ë°˜ ìº í•‘ ì§ˆë¬¸ì´ë¯€ë¡œ ë°”ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return {"route_decision": "end"}


def route_primary_classification(state: GraphState):
    """
    1ì°¨ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œë¥¼ ê²°ì •
    """
    if state['classification'] == 'ì¼ë°˜ ìº í•‘':
        # ì¼ë°˜ ìº í•‘ ì§ˆë¬¸ì€ ê²€ìƒ‰ ì—†ì´ ë°”ë¡œ ë‹µë³€ ìƒì„±
        return 'generate'
    elif state['classification'] == 'ì¥ì†Œ ì¶”ì²œ':
        # ì¥ì†Œ ì¶”ì²œ ì§ˆë¬¸ì€ ê²€ìƒ‰ ë…¸ë“œë¥¼ ê±°ì¹¨
        return 'retrieve'
    else: # ì˜ˆì™¸ ìƒí™©
        return 'generate'


# ==============================================================================
# 6. LangGraph ê·¸ë˜í”„ êµ¬ì„±
# ==============================================================================
workflow = StateGraph(GraphState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("primary_classify", primary_classify_question)
# 'retrieve' ë…¸ë“œ ì´ë¦„ì„ ë³€ê²½í•˜ì—¬ ì˜ë¯¸ë¥¼ ëª…í™•í•˜ê²Œ í•¨
workflow.add_node("retrieve", retrieve_context_async)
workflow.add_node("generate", generate_final_answer)
workflow.add_node("check_quality", check_answer_quality)

# ì‹œì‘ì  ì„¤ì •
workflow.set_entry_point("primary_classify")

# 1ì°¨ ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
workflow.add_conditional_edges(
    "primary_classify",
    route_primary_classification,
    {
        "generate": "generate",
        "retrieve": "retrieve"
    }
)

# 'retrieve' ë…¸ë“œì—ì„œ 'generate' ë…¸ë“œë¡œ ì´ë™
workflow.add_edge("retrieve", "generate")

# ë‹µë³€ ìƒì„± í›„ í’ˆì§ˆ ê²€ì‚¬
workflow.add_edge("generate", "check_quality")

# í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼ì— ë”°ë¼ ì¶”ê°€ ê²€ìƒ‰ ë˜ëŠ” ì¢…ë£Œ
workflow.add_conditional_edges(
    "check_quality",
    lambda state: state['route_decision'],
    {"continue": "retrieve", "end": END}
)

app = workflow.compile()

# ==============================================================================
# 7. í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ë° ë©”ì¸ ì‹¤í–‰
# ==============================================================================
def test_camping_qa(question: str):
    print("="*60)
    print(f"ì§ˆë¬¸: {question}")
    print("="*60)
    
    start_time = time.perf_counter()
    
    try:
        # ì´ˆê¸° ìƒíƒœì— locations í•„ë“œ ì¶”ê°€
        initial_state = {
            "question": question, 
            "loop_count": 0, 
            "context": [], 
            "locations": [],  # <--- ì´ˆê¸°í™”
            "search_attempted": False
        }
        result = asyncio.run(app.ainvoke(initial_state))
        
        end_time = time.perf_counter()
        
        print(f"\nğŸ¯ ìµœì¢… ë¶„ë¥˜: {result.get('classification', 'N/A')}")
        
        # ìœ„ì¹˜ ì •ë³´ ì¶œë ¥ ë¡œì§ ì¶”ê°€
        if result.get('locations'):
            print(f"\nğŸ“ ì¶”ì²œ ì¥ì†Œ (ì§€ë„ í‘œì‹œìš©):")
            for loc in result['locations']:
                print(f"     - {loc['name']} (ìœ„ë„: {loc['lat']}, ê²½ë„: {loc['lon']})")

        print(f"\nğŸ“ ìµœì¢… ë‹µë³€:")
        print("-" * 40)
        print(result['final_answer'])
        print("-" * 40)
        
        print(f"\nğŸ“Š ì‹¤í–‰ ì •ë³´:")
        print(f"     - ì „ì²´ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print(f"     - ì‚¬ìš©ëœ ì •ë³´ ì†ŒìŠ¤ ìˆ˜: {len(result.get('context', []))}")
        print(f"     - ë£¨í”„ íšŸìˆ˜: {result.get('loop_count', 0)}")
        
        if result.get('context'):
            print(f"\nğŸ“š ê²€ìƒ‰ëœ ì •ë³´ ì†ŒìŠ¤:")
            for i, doc in enumerate(result['context'][:3], 1):
                print(f"     {i}. {doc[:100]}..." if len(doc) > 100 else f"     {i}. {doc}")
        
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸ•ï¸ ê°œì„ ëœ ìº í•‘ Q&A ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    test_questions = [
        "ìº í•‘í•  ë•Œ í•„ìš”í•œ ì¤€ë¹„ë¬¼ì´ ë­ì•¼?",
        "ê°•ì›ë„ì—ì„œ ë…¸ì§€ ìº í•‘ í•  ìˆ˜ ìˆëŠ” ê³³ ì¶”ì²œí•´ì¤˜",
        "ê°€í‰ ê¸€ë¨í•‘ ì˜ˆì•½ ê°€ëŠ¥í•œ ê³³ ìˆì–´?",
        "ë¶€ì‚° ê·¼ì²˜ ì¹´ë¼ë°˜ ìº í•‘ì¥ ì–´ë””ê°€ ì¢‹ì•„?",
        "ê²¨ìš¸ ìº í•‘ í…íŠ¸ ê³ ë¥´ëŠ” ë°©ë²• ì•Œë ¤ì¤˜",
        "ì œì£¼ë„ ê¸€ë¨í•‘ ì¶”ì²œí•´ì¤˜",
        "ì˜¤ì§€ ìº í•‘ ê°ˆ ë•Œ ì£¼ì˜ì‚¬í•­ì€?"
    ]
    
    for i, question in enumerate(test_questions, 1):
        test_camping_qa(question)
        if i < len(test_questions):
            print("\n" + "="*60 + "\n")
    
    print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
