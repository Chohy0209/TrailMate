import os
import time
import torch
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# âœ… ì‚¬ìš©ì ì„¤ì • --------------------------------
model_id = "skt/A.X-4.0-Light"
adapter_root_dir = "D:/trailmate_checkpoints/lora_ax_light_qvproj"
eval_file = "eval_prompts.txt"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ğŸ”§ ìƒì„± í•˜ì´í¼íŒŒë¼ë¯¸í„°
max_new_tokens = 768
temperature = 0.5
top_p = 0.8
repetition_penalty = 1.3
do_sample = True

# âœ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = (
    "[ì—­í• ] ë‹¹ì‹ ì€ ìº í•‘ì„ ê°€ëŠ” ì‚¬ëŒë“¤ì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤. ìº í•‘ ê²½í—˜ê³¼ ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\n"
    "[ì§ˆë¬¸] {prompt}\n"
    "[ê¸ˆì§€] âš ï¸ ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ìº í•‘ê³¼ ë¬´ê´€í•œ ë‚´ìš©(ì˜ˆ: ì—˜ë¦¬ë² ì´í„°, ë„ì‹¬, ì¼ë°˜ ì¼ìƒìƒí™œ)ì€ ìƒì„±í•˜ì§€ ë§ì•„ì•¼ í•˜ë©°, ì´ëŸ° ë‚´ìš©ì€ ì‘ë‹µ í’ˆì§ˆ ì €í•˜ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n"
    "[ë‹µë³€ ìš”êµ¬ì‚¬í•­]\n"
    "1. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n"
    "2. ê´€ë ¨ëœ ìº í•‘ì¥, ì¥ì†Œ, ì¥ë¹„ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì„¸ìš”.\n"
    "3. ì‹¤ìš©ì ì¸ íŒì´ë‚˜ ì£¼ì˜ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”.\n"
    "4. ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ëª…í™•í•˜ì§€ ì•Šë‹¤ë©´ 'ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”.\n"
    "[ë‹µë³€]"
)

# âœ… tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
eos_token_id = tokenizer.eos_token_id

# âœ… í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
with open(eval_file, "r", encoding="utf-8") as f:
    prompts = [line.strip() for line in f if line.strip()]

# âœ… ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì •ë ¬
checkpoints = sorted([
    os.path.join(adapter_root_dir, d)
    for d in os.listdir(adapter_root_dir)
    if d.startswith("checkpoint-")
], key=lambda x: int(x.split("-")[-1]))

if not checkpoints:
    print("âŒ í‰ê°€í•  ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit()

# âœ… í‰ê°€ ë£¨í”„
for ckpt in checkpoints:
    checkpoint_name = os.path.basename(ckpt)
    log_dir = "evaluate"
    os.makedirs(log_dir, exist_ok=True)
    output_log_file = os.path.join(log_dir, f"evaluate-{checkpoint_name[11:]}.txt")

    print("=" * 100)
    print(f"ğŸ” Evaluating {checkpoint_name}")
    print("=" * 100)

    with open(output_log_file, "w", encoding="utf-8") as f:
        f.write(f"ğŸ” LoRA ì²´í¬í¬ì¸íŠ¸ í‰ê°€ - {checkpoint_name}\n\n")

    # âœ… ëª¨ë¸ ë¡œë”© (8bit)
    base_model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        trust_remote_code=True,
        load_in_8bit=True,
        low_cpu_mem_usage=True
    )
    model = PeftModel.from_pretrained(base_model, ckpt)
    model.eval()

    for i, prompt in enumerate(prompts):
        full_prompt = prompt_template.format(prompt=prompt)
        inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
        inputs.pop("token_type_ids", None)

        start = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                eos_token_id=eos_token_id,
                pad_token_id=eos_token_id
            )
        end = time.time()
        elapsed = end - start

        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = decoded.split("[ë‹µë³€]")[-1].strip()

        if len(answer.split()) < 10 or not answer.endswith((".", "ë‹¤", "ìš”")):
            print("âš ï¸ ì‘ë‹µì´ ì¤‘ê°„ì— ëŠê²¼ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            answer += "\n[âš ï¸ ë‹µë³€ì´ ì¤‘ê°„ì— ëŠê²¼ì„ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.]"

        print(f"\n[Q{i+1}] {prompt}")
        print(f"[A{i+1}] {answer}")
        print(f"ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")

        with open(output_log_file, "a", encoding="utf-8") as f:
            f.write(f"\n[Q{i+1}] {prompt}\n")
            f.write(f"[A{i+1}] {answer}\n")
            f.write(f"ğŸ•’ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ\n")

    used = torch.cuda.memory_allocated() / 1e9
    reserved = torch.cuda.memory_reserved() / 1e9
    print(f"\nğŸ§  VRAM ì‚¬ìš©ëŸ‰: {used:.2f} GB (ì˜ˆì•½: {reserved:.2f} GB)")

    del model
    del base_model
    torch.cuda.empty_cache()
    gc.collect()

    print(f"âœ… VRAM ì´ˆê¸°í™” ì™„ë£Œ after {checkpoint_name}\n")
    print("#" * 100 + "\n")
